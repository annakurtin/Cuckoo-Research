---
title: "JAGS_Detection_Model_HabChap"
author: "Anna Kurtin"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jagsUI)
library(tidyverse)
```


# Data Reading and Formatting
```{r Read in Data}
# y data: read in a dataframe that has a row for every point and a column for every detection period 

# Site-level covariates
# Read in vegetation density for each site
vegdense <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Detection_Covariates/ShrubTreeDensityComposite_2023_ARUPoints.csv")
vegdense <- vegdense %>%
  mutate(site_id = case_when(
    # If point_id has four letters then a dash then three numbers
    grepl("^[[:alpha:]]{4}-[[:digit:]]{3}$", point_id) ~ point_id,
    # If point_id has two numbers, three numbers, or three letters then a dash then one number
    grepl("^[[:digit:]]{2,3}-[[:digit:]]{1}$", point_id) |
      grepl("^[[:alpha:]]{3}-[[:digit:]]{1}$", point_id) ~ gsub("-.*", "", point_id)
  ))
# take the average of this across the site
vegdense_avg <- vegdense %>% group_by(site_id,sampling_design) %>% summarize(veg_density_avg = round(mean(composite_dense,na.rm = TRUE),2))

# background noise
background_db <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Detection_Covariates/BackgroundNoiseBandpass_2023_ARUPoints.csv")
colnames(background_db) <- c("site_id","backdb_survey1","backdb_survey2","backdb_survey3","backdb_survey4","backdb_survey5","backdb_survey6")


# Read in detection and effort data 
effort <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Detection_History/2023_All_ARUs/Outputs/DetectionHist_CamTrapR/bbcu__effort__not_scaled_14_days_per_occasion__2024-04-29.csv") %>% clean_names
survey_periods <- colnames(effort)
colnames(effort) <- c("site_session","effort_survey1","effort_survey2","effort_survey3","effort_survey4","effort_survey5","effort_survey6")
effort <- effort %>% separate(site_session, into = c("site_id","session_name"), sep = "__") %>% select(-session_name)

detections <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Detection_History/2023_All_ARUs/Outputs/DetectionHist_CamTrapR/bbcu__detection_history__with_effort__14_days_per_occasion__2024-04-29.csv") %>% clean_names
colnames(detections) <- c("site_session","det_survey1","det_survey2","det_survey3","det_survey4","det_survey5","det_survey6")

# Split apart the detections into site and point
detections <- detections %>% separate(site_session, into = c("site_id","session_name"), sep = "__")

# # Combine them
full_dat <- left_join(detections,background_db, by = "site_id")
full_dat <- left_join(full_dat,vegdense_avg, by = "site_id")
full_dat <- left_join(full_dat, effort, by = "site_id")
full_dat <- full_dat %>% mutate(start_date_s1 = 152,
                     start_date_s2 = 166,
                     start_date_s3 = 180,
                     start_date_s4 = 194,
                     start_date_s5 = 208,
                     start_date_s6 = 222)
full_dat <- full_dat %>% select(-c(session_name, sampling_design))
# Trying this to see if it will resolve the model conflicts 
# full_dat <- na.omit(full_dat)
saveRDS(full_dat, file ="./Data/Habitat_Model_Covariates/Full_DetectionData_JAGSModels_HabChap.Rdata")
```

# Data Exploration and Visualization
```{r Data Exploration}
# Didn't include sampling design bc in previous work these covaried 
# look at whether any others covary
## Veg Density and Average Background Noise
plot(x = full_dat$avg_db,y = full_dat$veg_density_avg)
## Date and survey effort
plot(x = full_dat$avg_db,y = full_dat$veg_density_avg)

# Visualize your data to inform your priors
# average db for survey session
# NEW average dB
db <- full_dat[,9:14]
db_long <- db %>% pivot_longer(cols = c(backdb_survey1,backdb_survey2,backdb_survey3,backdb_survey4,backdb_survey5,backdb_survey6), names_to = "survey_period",values_to = "db" )
hist(scale(db_long$db), xlab = "Background dB", main = "Scaled Histogram of Background Noise", col = eyering_red1)
## Looks like a normal distribution around the scaled variable should be good for a prior

# Veg density
hist(full_dat$veg_density_avg, xlab = "Average Composite Density at a Site", main = "Histogram of Vegetation Density", col = eyering_red1)
hist(scale(full_dat$veg_density_avg), xlab = "Average Composite Density at a Site", main = "Scaled Histogram of Vegetation Density", col = eyering_red1)
## This is a distribution with a left skew - gamma? but it goes below zero --> use normal instead?

# Date
# uniform distribution between 152-208 - does this include those values?
dates <- full_dat[,23:28]
dates_long <- dates %>% pivot_longer(cols = c(start_date_s1,start_date_s2,start_date_s3,start_date_s4,start_date_s5,start_date_s6), names_to = "survey_period",values_to = "date" )
hist(scale(dates_long$date), xlab = "Date", main = "Scaled Histogram of Survey Start Date", col = eyering_red1)

# Effort
efforts <- full_dat[,17:22]
efforts_long <- efforts %>% pivot_longer(cols = c(effort_survey1,effort_survey2,effort_survey3,effort_survey4,effort_survey5,effort_survey6), names_to = "survey_period",values_to = "effort" )
hist(scale(efforts_long$effort), xlab = "Effort", main = "Scaled Histogram of Survey Effort", col = eyering_red1)

```

# JAGS Model
```{r Run JAGS Model 1}
full_dat <- readRDS("./Data/Habitat_Model_Covariates/Full_DetectionData_JAGSModels_HabChap.Rdata")
# b1/cov1: date
# b2/cov2: veg density
# b3/cov3: background noise
# b4/cov4: effort

# you can just specify how to filll in the values
# Can also specify to skip the values that are missing
# assumption is that the data is missing at random
# pulls the output towards the center
# frequentist generally has to remove NAs from the dataset

# Create the jags_model
cat("
  model{
    # Loop through each sampling unit
    for(i in 1:n_unit){
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      
      cov2[i] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
      
      for(j in 1:n_rep){
        cov1[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov3[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov4[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        
        det_data[i,j] ~  dbern(Z[i]*theta[i,j])  # Create detection data drawn from a bernoulli distribution incorporating the true presence (Z) with probability of detection theta
        logit(theta[i,j]) <- b0 + b1*cov1[i,j] + b2*cov2[i] + b3*cov3[i,j] + b4*cov4[i,j]
        
      }
    }
    
    # Priors
    a0 ~ dnorm(0, 0.01)
    b0 ~ dnorm(0, 0.01)
    b1 ~ dunif(-2, 2)
    b2 ~ dnorm(0, 1)
    b3 ~ dnorm(0, 1)
    b4 ~ dnorm(0, 1)
  }
  ", file = "JAGS_Occ_DetectionModel1.txt")

# Pull out the detections so that you have n_unit and n_site
detections_only <- full_dat[,2:7]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat <- rowSums(detections_only, na.rm = T)
z_dat[z_dat > 1] <- 1
z_dat[z_dat == 0] <- NA
#z_dat

# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z_dat,
  # Detection/nondetection data
  det_data = detections_only,
  # Date
  cov1 = full_dat[,21:26],
  # Veg Density - site level
  cov2 = full_dat[,14],
  # Background noise
  cov3 = full_dat[,8:13],
  # Effort 
  cov4 = full_dat[,15:20],
  n_unit = nrow(detections_only),
  n_rep = ncol(detections_only)
)

# Set initial values (optional)
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = runif(1, min = -2, max = 2),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}

# Run JAGS model
jags_fit_global <- jags(data = jags_data, 
                 model.file = "JAGS_Occ_DetectionModel1.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = 10000, 
                 n.burnin = 2000, 
                 n.chains = 3,
                 inits = init_func)
#saveRDS(jags_fit_global, file = "./R_Scripts/7_Model_Scripts/JAGS_Global_Detection_Model.Rdata")

```


```{r look at JAGS Model 1 Results}
# Read in the model that we saved after running the above code
jags_fit_global <- readRDS("./R_Scripts/7_Model_Scripts/JAGS_Global_Detection_Model.Rdata")

# Get a summary of the results
summary(jags_fit_global)
# Looking at f values
# b1 date: 0.99 (strongly impacts)
# b2 veg density: 0.59 (weak-low impact)
# b3 background noise: 0.96 (strongly impacts)
# b4 effort:  0.88 (moderately impacts)

# Plot the results
plot(jags_fit_global)
# B0, B1, and B3 (with a small n_eff value) didn't converge

# Extract MCMC chains
#chains <- as.mcmc(jags_fit_global)

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#TODO 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Make violin plots from the posterior samples of the chains to visualize the posterior distribution and its overlap with zero (check the code that you wrote with thomas)
test <- extract(det_mod_global)
test$beta_det

# Look at the proportion that is on the same size of zero as the mean (derived from the summary of the model)
# make a violin plot (package vioplot I think)
# If the percent of posterior samples that fall on the same of zero as the mean is >90%, you have reason to believe there's a strong effect (F value in jags)
# don't go down to 70%
# Effect of effort 
nI <- length(test$beta_det[,4])
length(which(test$beta_det[,4] < 0))/nI


```


# Test GPT code - BAD DO NOT USE OVERLY COMPLICATED

```{r Run JAGS Model 2, eval=FALSE}
full_dat <- readRDS("./Data/Habitat_Model_Covariates/Full_DetectionData_JAGSModels_HabChap.Rdata")
# b1/cov1: date
# b2/cov2: veg density
# b3/cov3: background noise
# b4/cov4: effort

# Create the jags_model
cat("
  model{
    # Loop through each sampling unit
    for(i in 1:n_unit){
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      for(j in 1:n_rep){
        det_data[i,j] ~  dbern(Z[i]*theta[i,j])  # Create detection data drawn from a bernoulli distribution incorporating the true presence (Z) with probability of detection theta
        logit(theta[i,j]) <- b0 + b1*cov1[i,j] + b2*cov2[i] + b3*cov3[i,j] + b4*cov4[i,j]
      }
    }
    # Imputation for missing covariates
    for(i in 1:n_unit){
      for(j in 1:n_rep){
        # Impute cov1
        cov1[i,j] <- ifelse(is_na_cov1[i,j] == 1, imputed_cov1[i,j], cov1[i,j])
        imputed_cov1[i,j] ~ dnorm(0, 1)
        # Impute cov3
        cov3[i,j] <- ifelse(is_na_cov3[i,j] == 1, imputed_cov3[i,j], cov3[i,j])
        imputed_cov3[i,j] ~ dnorm(0, 1)
        # Impute cov4
        cov4[i,j] <- ifelse(is_na_cov4[i,j] == 1, imputed_cov4[i,j], cov4[i,j])
        imputed_cov4[i,j] ~ dnorm(0, 1)
      }
      # Impute cov2
      cov2[i] <- ifelse(is_na_cov2[i] == 1, imputed_cov2[i], cov2[i])
      imputed_cov2[i] ~ dnorm(0, 1)
    }
    # Priors
    a0 ~ dnorm(0, 0.01)
    b0 ~ dnorm(0, 0.01)
    b1 ~ dunif(-2, 2)
    b2 ~ dnorm(0, 1)
    b3 ~ dnorm(0, 1)
    b4 ~ dnorm(0, 1)
  }
  ", file = "JAGS_Occ_DetectionModel2_Imputed.txt")

# Pull out the detections so that you have n_unit and n_site
detections_only <- full_dat[,2:7]
# Create a list of data for JAGS
# Date
cov1 <- full_dat[,21:26]
# Veg Density - site level
cov2 <- full_dat[,14]
# Background noise
cov3 <- full_dat[,8:13]
# Effort 
cov4 <- full_dat[,15:20]

# Create indicator matrices for missing values
is_na_cov1 <- is.na(cov1)
is_na_cov2 <- is.na(cov2)
is_na_cov3 <- is.na(cov3)
is_na_cov4 <- is.na(cov4)

# Convert logical matrices to numeric (0/1)
is_na_cov1 <- ifelse(is_na_cov1, 1, 0)
is_na_cov2 <- ifelse(is_na_cov2, 1, 0)
is_na_cov3 <- ifelse(is_na_cov3, 1, 0)
is_na_cov4 <- ifelse(is_na_cov4, 1, 0)

# Create a list of data for JAGS
jags_data <- list(
  det_data = detections_only,
  cov1 = cov1,
  cov2 = cov2,
  cov3 = cov3,
  cov4 = cov4,
  is_na_cov1 = is_na_cov1,
  is_na_cov2 = is_na_cov2,
  is_na_cov3 = is_na_cov3,
  is_na_cov4 = is_na_cov4,
  n_unit = nrow(detections_only),
  n_rep = ncol(detections_only)
)

# Set initial values (optional)
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = runif(1, min = -2, max = 2),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}

# Run JAGS model
jags_fit_global <- jags(data = jags_data, 
                 model.file = "JAGS_Occ_DetectionModel2_Imputed.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = 10000, 
                 n.burnin = 2000, 
                 n.chains = 3,
                 inits = init_func)

jags_fit_global
```
