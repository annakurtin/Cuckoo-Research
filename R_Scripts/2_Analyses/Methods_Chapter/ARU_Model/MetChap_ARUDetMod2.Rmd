---
title: "Methods Chapter ARU Detection Model"
author: "Anna Kurtin"
date: "2024-08-12"
output: html_document
---

This is the model I will use for the ARU detection probability. This is the same model structure as HabChap DetMod6 with vegetation density added. 


**Model structure:**

Naive state model:
$$
z_i \sim Bernoulli(\psi_i)
$$ 
$$
logit(\psi_i) = \alpha_0
$$

Process model with four covariates:  

$$
y_{ij} \sim Bernoulli(p_{ij} * z_i)
$$

$$
logit(p_{ij}) = \beta_0 + \beta_{1}*date + \beta_{2}^2 * date + \beta_{3} * avg db + \beta_{4}*effort + \beta_{5}*veg_dense
$$

**Handling of NAs:** Skip for missing detections, imputate missing covariates.

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jagsUI)
library(rjags)
library(tidyverse)
library(jagshelper)
library(ggplot2)
library(corrgram)
library(beepr)
source("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/5_Visualization/Create_HexCodes_CuckooColorBlind.R")
```

Load in formatted data

```{r Load in Data}
#full_dat <- readRDS(file ="C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Full_DetectionDataSCALED_JAGSModels_HabChap.Rdata")
full_dat <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/HabChap_DetOccCovsFull_SCALED_8-12.csv")
# drop 8 through 23 for detection data
det_dat <- full_dat[,-c(8:23)]

detections <- full_dat[,2:7]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat <- rowSums(detections, na.rm = T)
z_dat[z_dat > 1] <- 1
z_dat[z_dat == 0] <- NA

# get the length of non-nas in the detection data
get_na <- function(x){(max(which(!is.na(x))))}
# Apply this to our data
miss <- apply(detections, 1, get_na)
```

```{R Look at Covariance, warnings = FALSE}
# Pivot this data long to be able to compare 
dates <- select(det_dat, site_id, 21:26)
dates_long <- dates %>% pivot_longer(cols = c(start_date_s1,
                                              start_date_s2,
                                              start_date_s3,
                                              start_date_s4,
                                              start_date_s5,
                                              start_date_s6), names_to = "survey_period",values_to = "date" )
# rename to s1, s2, etc
dates_long <- dates_long %>% mutate(survey_period = case_when(survey_period == "start_date_s1" ~ "s1",
                                                survey_period == "start_date_s2" ~ "s2",
                                                survey_period == "start_date_s3" ~ "s3",
                                                survey_period == "start_date_s4" ~ "s4",
                                                survey_period == "start_date_s5" ~ "s5",
                                                survey_period == "start_date_s6" ~ "s6"))
# combine
dates_long <- unite(dates_long,site_survey,c(site_id, survey_period),sep="_",remove=TRUE)

db <- select(det_dat, site_id,8:13)
db_long <- db %>% pivot_longer(cols = c(backdb_survey1,
                                        backdb_survey2,
                                        backdb_survey3,
                                        backdb_survey4,
                                        backdb_survey5,
                                        backdb_survey6), names_to = "survey_period",values_to = "db" )
db_long <- db_long %>% mutate(survey_period = case_when(survey_period == "backdb_survey1" ~ "s1",
                                                survey_period == "backdb_survey2" ~ "s2",
                                                survey_period == "backdb_survey3" ~ "s3",
                                                survey_period == "backdb_survey4" ~ "s4",
                                                survey_period == "backdb_survey5" ~ "s5",
                                                survey_period == "backdb_survey6" ~ "s6"))
db_long <- unite(db_long,site_survey,c(site_id, survey_period),sep="_",remove=TRUE)

effort <- select(det_dat, site_id, 15:20)
effort_long <- effort %>% pivot_longer(cols = c(effort_survey1,
                                                effort_survey2,
                                                effort_survey3,
                                                effort_survey4,
                                                effort_survey5,
                                                effort_survey6), names_to = "survey_period",values_to = "effort" )
effort_long <- effort_long %>% mutate(survey_period = case_when(survey_period == "effort_survey1" ~ "s1",
                                                survey_period == "effort_survey2" ~ "s2",
                                                survey_period == "effort_survey3" ~ "s3",
                                                survey_period == "effort_survey4" ~ "s4",
                                                survey_period == "effort_survey5" ~ "s5",
                                                survey_period == "effort_survey6" ~ "s6"))
effort_long <- unite(effort_long,site_survey,c(site_id, survey_period),sep="_",remove=TRUE)

covs_long <- left_join(dates_long, db_long, by = "site_survey") %>% left_join(., effort_long, by = "site_survey")

corrgram(covs_long, order=TRUE, lower.panel=panel.cor,
         upper.panel=panel.pts, text.panel=panel.txt,
         main="Correlations in ARU Detection Metrics")

# Include veg density in this comparison 
# Reformat your long data
lon2 <- covs_long %>% separate(site_survey, into = c("site_id","survey"), sep = "_") %>% select(-survey) %>% group_by(site_id) %>% summarize(date_avg = mean(date, na.rm = TRUE), db_avg = mean(db, na.rm = TRUE), effort_avg = mean(effort, na.rm = TRUE))
# pull out veg values
veg <- select(det_dat, site_id, veg_density_avg_scaled)
# combine them for comparison
covs2 <- left_join(lon2, veg, by = "site_id")
covs2 <- covs2 %>% select(-date_avg)

corrgram(covs2, order=TRUE, lower.panel=panel.cor,
         upper.panel=panel.pts, text.panel=panel.txt,
         main="Correlations in ARU Detection Metrics")
# veg data isn't colinear with the other covariates
```


```{R}
# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z_dat,
  # Detection/nondetection data
  det_data = detections,
  # Date
  covb1 = dates[,2:7],
  # Background noise
  covb3 = db[,2:7],
  # Effort 
  covb4 = effort[,2:7],
  covb5 = veg[,2],
  n_unit = nrow(detections),
  miss = miss
)

# Set initial values 
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = rnorm(1, 0, 1),
    b1Q = rnorm(1, 0 ,1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1),
    b5 = rnorm(1, 0, 1)
  )
}
```



```{R Create Model, eval = FALSE}
cat("
  model{
      # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dlogis(0, 1)
    b1Q ~ dlogis(0, 1)
    b3 ~ dlogis(0, 1)
    b4 ~ dlogis(0, 1)
    b5 ~ dlogis(0, 1)
    
    # Loop through each sampling unit
    for(i in 1:n_unit){
      covb5[i] ~ dnorm(0, 1)
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      
      
      for(j in 1:miss[i]){
        covb1[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        covb3[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        covb4[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        
        det_data[i,j] ~  dbern(mu_p[i,j])  # Create probability density drawn for each site
        mu_p[i,j] <- Z[i]*theta[i,j]
        logit(theta[i,j]) <- p[i,j]
        p[i,j] <- b0 + b1*covb1[i,j] + b1Q*(covb1[i,j]^2) + b3*covb3[i,j] + b4*covb4[i,j] + b5*covb5[i]
        # Det NA
        det_na[i,j] <- det_data[i,j]
  
        #Calculate Bayes p-value - plug the parameters into the equation for each sample of the posterior and use this to create new observations
        Presi[i,j] <- (det_na[i,j] - theta[i,j])^2 # Calculate squared residual of observed data
        y_new[i,j] ~ dbern(mu_p[i,j])             # Simulate observed data
        Presi_new[i,j] <- (y_new[i,j] - theta[i,j])^2 # Calculate squared residual error of simulated data
        
      }
    }
    # Derived parameters
    ## Calculate sum of squared residual errors for observed 
    for(i in 1:n_unit){
       sobs[i] <- sum(Presi[i,1:miss[i]])
       ssim[i] <- sum(Presi_new[i,1:miss[i]])
    }
    SSEobs <- sum(sobs[])
    SSEsim <- sum(ssim[])
    p_val <- step(SSEsim - SSEobs) 
  } 
  ", file = "C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Methods_Chapter/ARU_Model/Model_Structure_Files/JAGS_ARUMod2.txt")
```

```{R Run Model, eval= FALSE, results = FALSE}
fit_2 <- jags(data = jags_data, 
                 model.file = "C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Methods_Chapter/ARU_Model/Model_Structure_Files/JAGS_ARUMod2.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b1Q", "b3", "b4", "b5", "p_val"),
                 n.iter = 10000, 
                 n.burnin = 5000, 
                 n.chains = 3,
                 n.thin = 2,
                 inits = init_func)
beep(sound = 3)
```

```{R Save Model, eval = FALSE, echo = FALSE}
saveRDS(fit_2, "C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Methods_Chapter/ARU_Model/Models_Ran_Outputs/MetChap_ARUMod2.Rdata")
```

### Model outputs: 
```{R}
fit_2 <- readRDS("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Methods_Chapter/ARU_Model/Models_Ran_Outputs/MetChap_ARUMod2.Rdata")
summary(fit_2)
tracedens_jags(fit_2, parmfrow = c(3,4))
```

**Reminder of covariates:**

* covb1 = date

* covb1Q: quadratic effect of date

* covb3: background noise

* covb4: effort

* covb5: veg density 


```{R Create Violin Plots}
# Plot posterior distribution
chains_m <- jags_df(fit_2)
# Select the chains for our covariates of interest
chains_viol <- chains_m %>% select(b1,b1Q,b3,b4, b5)
# Rename them to be more interpretable
colnames(chains_viol) <- c("date","quadratic date","background_noise","effort", "veg_density")
# Pivot this longer so that we can visualize it
chains_viol_long <- chains_viol %>% pivot_longer(cols = c("date","quadratic date","background_noise","effort", "veg_density"),names_to = "Parameter", values_to = "Values")
# Select the median values and associate them with their names
med_vals <- data.frame(vals = c(fit_2$q50$b1,fit_2$q50$b1Q, fit_2$q50$b3, fit_2$q50$b4, fit_2$q50$b5), name = c("date","quadratic date","background_noise","effort", "veg_density"))

# Create the violin plot
ggplot() +
  geom_violin(data=chains_viol_long, aes(x = Parameter, y = Values,fill = Parameter),trim = FALSE) +
  geom_point(data = med_vals, aes(x = name, y = vals)) +
  labs(title = "Posterior Distribution of Detection Covariates") +
  scale_fill_manual(values = c("date"=palette_5[1], "quadratic date" = palette_5[5],"background_noise"=palette_5[2], "effort" = palette_5[3], "veg_density" = palette_5[4])) + 
  geom_hline(yintercept = 0, linetype = "solid", color = "black") +
  theme_minimal()
```


Detection probability estimates:

```{R}
p <- round(plogis(fit_2$q50$b0),4)
print(paste0("Estimate for detection probability of a single ARU survey:",p))

n <- 6
p_star <- round(1 - (1 - p)^n, 4)
print(paste0("Estimate for cumulative ARU detection probability:",p_star))
# how does this line up with our naive estimates of how many sites had detections on ARU vs PB?
pb_dat <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Playback_Results/2023/Outputs/2023_PBData_FormatforOccModSCALED_8-6-24.csv")
pb_dat$bbcu_pb <- apply(pb_dat[, 2:4], 1, max, na.rm = TRUE)
pb <- pb_dat %>% select(site_id, bbcu_pb)
#sum(pb$bbcu_pb)

full_dat$bbcu_aru <- apply(full_dat[,2:7], 1, max, na.rm = TRUE)
aru <- full_dat %>% select(site_id, bbcu_aru)
#sum(aru$bbcu_aru)
comp_dat <- inner_join(aru,pb, by = "site_id")

comp_dat$pres <- apply(comp_dat[, 2:3], 1, max, na.rm = TRUE)
# how many sites truly had cuckoos?
sum(comp_dat$pres)

# What is the naive estimate for detection probability for each method?
# Naive occupancy estimate
naiveocc_aru <- (sum(comp_dat$bbcu_aru)/nrow(comp_dat)) * 100
naivedet_aru <- round((sum(comp_dat$bbcu_aru)/sum(comp_dat$pres)) * 100, 2)
naiveocc_pb <- (sum(comp_dat$bbcu_pb)/nrow(comp_dat)) * 100
naivedet_pb <- (sum(comp_dat$bbcu_pb)/sum(comp_dat$pres)) * 100
print(paste0("Naive detection estimate:", naivedet_aru))
# what would the value for occupancy with playbacks be if we corrected it by the detection?
```

*Thoughts and lingering questions:*

* Including vegetation density in this model: the f statistic changed from the model that didn't have the quadratic effect of date. 

* F statistics changed for veg density and effort. Veg density f statistic increased relative to the model without a quadratic effect on date, and survey effort f statistic decreased relative to model without quadratic effect on date. 

* When we compare aru vs playback detections, are we comparing the same thing??