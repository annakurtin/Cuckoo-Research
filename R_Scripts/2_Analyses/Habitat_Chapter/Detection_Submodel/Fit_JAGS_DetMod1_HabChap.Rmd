---
title: "Habitat Chapter JAGS Detection Sub Model 1"
author: "Anna Kurtin"
date: "5/20/2024"
output: html_document
---



This is the original detection submodel I built. 

*Model structure:*

Naive state model
$$
z_i \sim Bernoulli(\psi)
$$ 
Process model with four covariates
$$
y_{ij} \sim Bernoulli(p| \beta_{date},\beta_{veg density},\beta_{avg db}, \beta_{effort})
$$
*Handling of NA data:* imputation

This model has improperly specified priors, which were fixed in Detection Submodel 2. There are several other models where I play around with the initial values and removing the last survey period before I figured out that my issue was that my priors were specified improperly.  



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jagsUI)
library(tidyverse)
library(jagshelper)
library(ggplot2)
library(coda)
source("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/5_Visualization/Create_HexCodes_CuckooColorBlind.R")
```

Shoutout to Matt Tyers for the package on interfacing with jagsUI:

[PDF](https://cran.r-project.org/web/packages/jagshelper/jagshelper.pdf)

[GitHub](https://github.com/mbtyers/jagshelper)


# Data Reading and Formatting

Read in data and format it for use in JAGS. 
```{r Read in Data, eval = FALSE}
# y data: read in a dataframe that has a row for every point and a column for every detection period 

# Site-level covariates
# Read in vegetation density for each site
vegdense <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Detection_Covariates/ShrubTreeDensityComposite_2023_ARUPoints.csv")
vegdense <- vegdense %>%
  mutate(site_id = case_when(
    # If point_id has four letters then a dash then three numbers
    grepl("^[[:alpha:]]{4}-[[:digit:]]{3}$", point_id) ~ point_id,
    # If point_id has two numbers, three numbers, or three letters then a dash then one number
    grepl("^[[:digit:]]{2,3}-[[:digit:]]{1}$", point_id) |
      grepl("^[[:alpha:]]{3}-[[:digit:]]{1}$", point_id) ~ gsub("-.*", "", point_id)
  ))
# take the average of this across the site
vegdense_avg <- vegdense %>% group_by(site_id,sampling_design) %>% summarize(veg_density_avg = round(mean(composite_dense,na.rm = TRUE),2))

# background noise
background_db <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Detection_Covariates/BackgroundNoiseBandpass_2023_ARUPoints.csv")
colnames(background_db) <- c("site_id","backdb_survey1","backdb_survey2","backdb_survey3","backdb_survey4","backdb_survey5","backdb_survey6")


# Read in detection and effort data 
effort <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Detection_History/2023_All_ARUs/Outputs/DetectionHist_CamTrapR/bbcu__effort__not_scaled_14_days_per_occasion__2024-04-29.csv") %>% clean_names
survey_periods <- colnames(effort)
colnames(effort) <- c("site_session","effort_survey1","effort_survey2","effort_survey3","effort_survey4","effort_survey5","effort_survey6")
effort <- effort %>% separate(site_session, into = c("site_id","session_name"), sep = "__") %>% select(-session_name)

detections <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Detection_History/2023_All_ARUs/Outputs/DetectionHist_CamTrapR/bbcu__detection_history__with_effort__14_days_per_occasion__2024-04-29.csv") %>% clean_names
colnames(detections) <- c("site_session","det_survey1","det_survey2","det_survey3","det_survey4","det_survey5","det_survey6")

# Split apart the detections into site and point
detections <- detections %>% separate(site_session, into = c("site_id","session_name"), sep = "__")

# # Combine them
full_dat <- left_join(detections,background_db, by = "site_id")
full_dat <- left_join(full_dat,vegdense_avg, by = "site_id")
full_dat <- left_join(full_dat, effort, by = "site_id")
full_dat <- full_dat %>% mutate(start_date_s1 = 152,
                     start_date_s2 = 166,
                     start_date_s3 = 180,
                     start_date_s4 = 194,
                     start_date_s5 = 208,
                     start_date_s6 = 222)
full_dat <- full_dat %>% select(-c(session_name, sampling_design))

# center and scale variables
test <- scale(full_dat$veg_density_avg)

#saveRDS(full_dat, file ="./Data/Habitat_Model_Covariates/Full_DetectionData_JAGSModels_HabChap.Rdata")
```

# Data Exploration and Visualization

Looking at distributions of data to inform what probability distribution I should use for priors. 
```{r Data Exploration}
full_dat <- readRDS(file ="C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Full_DetectionData_JAGSModels_HabChap.Rdata")
# Didn't include sampling design bc in previous work these covaried 
# look at whether any others covary
## Veg Density and Average Background Noise
#plot(x = full_dat$avg_db,y = full_dat$veg_density_avg)
## Date and survey effort
#plot(x = full_dat$avg_db,y = full_dat$veg_density_avg)

# Visualize your data to inform your priors
# average db for survey session
# NEW average dB
db <- full_dat[,8:13]
db_long <- db %>% pivot_longer(cols = c(backdb_survey1,backdb_survey2,backdb_survey3,backdb_survey4,backdb_survey5,backdb_survey6), names_to = "survey_period",values_to = "db" )
hist(scale(db_long$db), xlab = "Background dB", main = "Scaled Histogram of Background Noise", col = cuckoo_palette[1])
## Looks like a normal distribution around the scaled variable should be good for a prior

# Veg density
hist(full_dat$veg_density_avg, xlab = "Average Composite Density at a Site", main = "Histogram of Vegetation Density", col = eyering_red1)
hist(scale(full_dat$veg_density_avg), xlab = "Average Composite Density at a Site", main = "Scaled Histogram of Vegetation Density", col = cuckoo_palette[1])
## This is a distribution with a left skew - gamma? but it goes below zero --> use normal instead?

# Date
# uniform distribution between 152-208 - does this include those values?
dates <- full_dat[,21:26]
dates_long <- dates %>% pivot_longer(cols = c(start_date_s1,start_date_s2,start_date_s3,start_date_s4,start_date_s5,start_date_s6), names_to = "survey_period",values_to = "date" )
min(scale(dates_long$date)) # -1.46
max(scale(dates_long$date)) # 1.46
hist(scale(dates_long$date), xlab = "Date", main = "Scaled Histogram of Survey Start Date", col = cuckoo_palette[1])

# Effort
efforts <- full_dat[,15:20]
efforts_long <- efforts %>% pivot_longer(cols = c(effort_survey1,effort_survey2,effort_survey3,effort_survey4,effort_survey5,effort_survey6), names_to = "survey_period",values_to = "effort" )
hist(scale(efforts_long$effort), xlab = "Effort", main = "Scaled Histogram of Survey Effort", col = cuckoo_palette[1])

```

# JAGS Model

## Formulas: 
**Global Detection Model and Priors:** 
$$
z_i \sim Bernoulli(\psi)
$$ 

$$
y_{ij} \sim Bernoulli(p| \beta_{date},\beta_{veg density},\beta_{avg db}, \beta_{effort})
$$

$$x_{avgdb} \sim normal(\mu = 0, \sigma = 1)$$
$$x_{vegdensity} \sim normal(\mu = 0, \sigma = 1)$$
$$x_{date} \sim uniform(lower = -2, upper = 2)$$
$$x_{effort} \sim normal(\mu = 0, \sigma = 1)$$
## Fit Model

Model 1: Global detection model with all covariates on my data

```{r Create JAGS Model 1, eval = FALSE}
# b1/cov1: date
# b2/cov2: veg density
# b3/cov3: background noise
# b4/cov4: effort

# We have some NA values in our data from ARUs that cut out early:
# you can just specify how to fill in the values
# Can also specify to skip the values that are missing (in other script)
# assumption is that the data is missing at random (met here, ARU model was randomized so no systemic bias in which ones went out)
# Filling in the data from a distribution pulls the output towards the center
# frequentist approaches generally have to remove NAs from the dataset - would remove 17 sites from our data

# Create the jags_model
cat("
  model{
    # Loop through each sampling unit
    for(i in 1:n_unit){
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      
      cov2[i] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
      
      for(j in 1:n_rep){
        cov1[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov3[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov4[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        
        det_data[i,j] ~  dbern(Z[i]*theta[i,j])  # Create detection data drawn from a bernoulli distribution incorporating the true presence (Z) with probability of detection theta
        logit(theta[i,j]) <- b0 + b1*cov1[i,j] + b2*cov2[i] + b3*cov3[i,j] + b4*cov4[i,j]
        
      }
    }
    
    # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dunif(-2, 2)
    b2 ~ dnorm(0, 1)
    b3 ~ dnorm(0, 1)
    b4 ~ dnorm(0, 1)
  }
  ", file = "JAGS_Occ_DetectionModel1.txt")
```
```{R Run JAGS Model 1}
# Pull out the detections so that you have n_unit and n_site
detections_only <- full_dat[,2:7]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat <- rowSums(detections_only, na.rm = T)
z_dat[z_dat > 1] <- 1
z_dat[z_dat == 0] <- NA
#z_dat

# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z_dat,
  # Detection/nondetection data
  det_data = detections_only,
  # Date
  cov1 = full_dat[,21:26],
  # Veg Density - site level
  cov2 = full_dat[,14],
  # Background noise
  cov3 = full_dat[,8:13],
  # Effort 
  cov4 = full_dat[,15:20],
  n_unit = nrow(detections_only),
  n_rep = ncol(detections_only)
)

# Set initial values (optional)
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = runif(1, min = -2, max = 2),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}

# Run JAGS model
jags_fit_global <- jags(data = jags_data, 
                 model.file = "JAGS_Occ_DetectionModel1.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = 10000, 
                 n.burnin = 2000, 
                 n.chains = 3,
                 inits = init_func)
#saveRDS(jags_fit_global, file = "./R_Scripts/7_Model_Scripts/JAGS_Global_Detection_Model.Rdata")

# From Thomas
# > tmp <- rnorm(10000, 0, 10)
# > hist(tmp)
# > hist(plogis(tmp))
# > tmp <- rlogis(10000, 0, 1)
# > hist(tmp)
# > hist(plogis(tmp))
```

# Evaluate Model Convergence and Output for Initial Model

```{r look at JAGS Model 1 Results}
# b1/cov1: date
# b2/cov2: veg density
# b3/cov3: background noise
# b4/cov4: effort

# Read in the model that we saved after running the above code
jags_fit_global <- readRDS("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Actual_Data/JAGS_Global_Detection_Model.Rdata")
print(jags_fit_global)
#traceplot(jags_fit_global)
# I think I need to try saving the mcmc chains instead
jags_chains_global <- jags_df(jags_fit_global)
# Look at the traceplot from coda
jags_fit_mcmc <- as.mcmc(jags_fit_global)
traceplot(jags_fit_mcmc)
# Only takes mcmc objects
#jags_chains_global$b1
# Get a summary of the results
summary(jags_fit_global)
## How do you transform these outputs into probability of detection???
# plot traceplots
tracedens_jags(jags_fit_global)
```
```{R Looking at F Values Old, include = FALSE}
#jags_fit_global$overlap0
# Looking at f values
# b1 date: 0.99 (strongly impacts)
# b2 veg density: 0.59 (weak-low impact)
# b3 background noise: 0.96 (strongly impacts)
# b4 effort:  0.88 (moderately impacts)

# Plot the results
#plot(jags_chains_global)
# B0, B1 (Date), and B3 (background noise) (with a small n_eff value) didn't converge
```

# Evaluating Posterior Distributions 

```{R Assess Posterior Distribution}
# Make violin plots from the posterior samples of the chains to visualize the posterior distribution and its overlap with zero (check the code that you wrote with thomas)
chains_viol <- jags_chains_global %>% select(b1,b2,b3,b4)
colnames(chains_viol) <- c("date","veg_density","background_noise","effort")
chains_viol_long <- chains_viol %>% pivot_longer(cols = c("date","veg_density","background_noise","effort"),names_to = "Parameter", values_to = "Values")

# Create the violin plot
ggplot(chains_viol_long, aes(x = Parameter, y = Values,fill = Parameter)) +
  geom_violin(trim = FALSE) +
  labs(title = "Violin Plot of MCMC Chains") +
  scale_fill_manual(values = c("date"=col1, "background_noise"=col2, "effort" = col3, "veg_density" = col4)) +
  theme_minimal()

# Establish the number of iterations
nI <- length(chains_viol$date)

# Create histograms of posterior distributions
hist(chains_viol$date, xlab = "Date", main = "Histogram of Posterior Distribution of Date", col = col1)
abline(v = 0, lwd = 3, col = "black")
# Print out the f value
jags_fit_global$mean$b1
# Negative, so we're testing which side of zero it falls on
date_fval <- length(which(jags_chains_global$b1 < 0))/nI
print(paste("F value for Date:", date_fval))

# Veg density
hist(chains_viol$veg_density, xlab = "Veg Density", main = "Histogram of Posterior Distribution of Veg Density", col = col4)
abline(v = 0,lwd = 3, col = "black")
jags_fit_global$mean$b2
# Positive, so we're testing which side of zero it falls on
veg_fval <- length(which(jags_chains_global$b2 > 0))/nI
print(paste("F value for Veg Density:", veg_fval))

# Background Noise
hist(chains_viol$background_noise, xlab = "Survey Level Background Noise", main = "Histogram of Posterior Distribution of Background Noise", col = col2)
abline(v = 0,lwd = 3, col = "black")
jags_fit_global$mean$b3
# Negative, so we're testing which side of zero it falls on
db_fval <- length(which(jags_chains_global$b3 < 0))/nI
print(paste("F value for Background Noise:", db_fval))

# Survey Effort
hist(chains_viol$effort, xlab = "Survey Level Effort", main = "Histogram of Posterior Distribution of Effort", col = col3)
abline(v = 0,lwd = 3, col = "black")
jags_fit_global$mean$b4
# Positive, so we're testing which side of zero it falls on
effort_fval <- length(which(jags_chains_global$b4 > 0))/nI
print(paste("F value for Effort:", effort_fval))

```

# Remaining Questions:

 - Which detection covariates to carry forward into my full model?

- What does it mean that some of the model didn't converge? Can I fix this or should I not rely on the output from this model?


# Running the Same Model Without the End Period
```{R Omitting End of Period}
# How many NAs are there?
full_dat %>% filter(is.na(det_survey1)|is.na(det_survey2)|is.na(det_survey3)|is.na(det_survey4)|is.na(det_survey5)|is.na(det_survey6))
# 16 rows
# What about in each survey period?
nrow(full_dat %>% filter(is.na(det_survey1)))
nrow(full_dat %>% filter(is.na(det_survey2)))
nrow(full_dat %>% filter(is.na(det_survey3)))
nrow(full_dat %>% filter(is.na(det_survey4)))
nrow(full_dat %>% filter(is.na(det_survey5)))
nrow(full_dat %>% filter(is.na(det_survey6)))
# adding about three NAs for each survey period

# Try removing the last two?
# Try removing different amounts of data and seeing how it affects your estimates and uncertainty

# Removing just the last survey period
full_dat_1rem <- full_dat %>% select(-matches("6"))

# Pull out the detections so that you have n_unit and n_site
detections_only_1rem <- full_dat_1rem[,2:6]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat_1rem <- rowSums(detections_only_1rem, na.rm = T)
z_dat_1rem[z_dat_1rem > 1] <- 1
z_dat_1rem[z_dat_1rem == 0] <- NA
#z_dat

# Create a list of data for JAGS
jags_data_1rem <- list(
  # True presence at sites with confirmed presences
  Z = z_dat_1rem,
  # Detection/nondetection data
  det_data = detections_only_1rem,
  # Date
  cov1 = as.matrix(full_dat_1rem %>% select(matches("date"))),
  # Veg Density - site level
  cov2 = full_dat_1rem[,12],
  # Background noise
  cov3 = as.matrix(full_dat_1rem %>% select(matches("backdb"))),
  # Effort 
  cov4 = as.matrix(full_dat_1rem %>% select(matches("effort"))),
  n_unit = nrow(detections_only_1rem),
  n_rep = ncol(detections_only_1rem)
)
# Set initial values (optional)
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = runif(1, min = -2, max = 2),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}
# Run JAGS model
# jags_fit_global_1rem <- jags(data = jags_data_1rem, 
#                  model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models/JAGS_Occ_DetectionModel1.txt",
#                  parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
#                  n.iter = 10000, 
#                  n.burnin = 2000, 
#                  n.chains = 3,
#                  inits = init_func)
saveRDS(jags_fit_global_1rem, file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Actual_Data/JAGS_Global_Detection_Model_LastSurveyRemoved.Rdata")
#jags_chains_global <- jags_df(jags_fit_global_1rem)
tracedens_jags(jags_fit_global_1rem)
summary(jags_fit_global_1rem)

diagnose_model(jags_fit_global_1rem)
```
This just further reduced the effective sample sizes and didn't help with convergence.


What to do when the model is not converging: 
- try running it for more iterations
- specify more precise initial values

```{r Increasing Iterations}
# jags_fit_global_1rem2 <- jags(data = jags_data_1rem, 
#                  model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models/JAGS_Occ_DetectionModel1.txt",
#                  parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
#                  n.iter = 20000, 
#                  n.burnin = 2000, 
#                  n.chains = 3,
#                  inits = init_func)
saveRDS(jags_fit_global_1rem2, file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Actual_Data/JAGS_Global_Detection_Model_LastSurveyRemoved_2xIter.Rdata")
tracedens_jags(jags_fit_global_1rem2)
summary(jags_fit_global_1rem2)
```
More iterations: didn't help, model looks maybe closer to converging but isn't there. I think I need to specify better initial values or thin the values to reduce correlation. 

```{R Increase Thinning}
# Run JAGS model
jags_fit_global_5xthin <- jags(data = jags_data, 
                 model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_Occ_DetectionModel1.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = 20000, 
                 n.burnin = 2000, 
                 n.chains = 3,
                 n.thin = 5,
                 inits = init_func)
saveRDS(jags_fit_global_5xthin, file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Actual_Data/JAGS_Global_Detection_Model_5xThin.Rdata")
print(jags_fit_global_5xthin)
tracedens_jags(jags_fit_global_5xthin)
summary(jags_fit_global_5xthin)
print(jags_fit_global)
```
How do I choose which initial values to use?


## Trying again with new initial values
```{R Trying Model with Different Initial Values}
# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z_dat,
  # Detection/nondetection data
  det_data = detections_only,
  # Date
  cov1 = full_dat[,21:26],
  # Veg Density - site level
  cov2 = full_dat[,14],
  # Background noise
  cov3 = full_dat[,8:13],
  # Effort 
  cov4 = full_dat[,15:20],
  n_unit = nrow(detections_only),
  n_rep = ncol(detections_only)
)

# Set initial values (optional)
init_func2 <- function(){
  list(
    a0 = .2, #set this to what our estimate for occupancy is
    b0 = .3, # set this to what I expect detection probability is
    b1 = runif(1, min = -2, max = 2),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}

# Run JAGS model
jags_fit_global_newinits <- jags(data = jags_data, 
                 model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_Occ_DetectionModel1.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = 10000, 
                 n.burnin = 2000, 
                 n.chains = 3,
                 inits = init_func2)
print(jags_fit_global_newinits)
tracedens_jags(jags_fit_global_newinits)
```

I keep getting this error "node inconsistent with parameters". I'm troubleshooting this based off of [this page](https://stackoverflow.com/questions/71174973/in-rjags-runjags-what-causes-the-node-inconsistent-with-parents-error-when-us).

```{R Change Priors}
# Create the jags_model
cat("
  model{
    # Loop through each sampling unit
    for(i in 1:n_unit){
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      
      cov2[i] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
      
      for(j in 1:n_rep){
        cov1[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov3[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov4[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        
        det_data[i,j] ~  dbern(Z[i]*theta[i,j])  # Create detection data drawn from a bernoulli distribution incorporating the true presence (Z) with probability of detection theta
        logit(theta[i,j]) <- b0 + b1*cov1[i,j] + b2*cov2[i] + b3*cov3[i,j] + b4*cov4[i,j]
        
      }
    }
    
    # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dnorm(0, 1)
    b2 ~ dnorm(0, 1)
    b3 ~ dnorm(0, 1)
    b4 ~ dnorm(0, 1)
  }
  ", file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_Occ_DetectionModel1_ModifiedDatePriorInit.txt")

# Pull out the detections so that you have n_unit and n_site
detections_only <- full_dat[,2:7]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat <- rowSums(detections_only, na.rm = T)
z_dat[z_dat > 1] <- 1
z_dat[z_dat == 0] <- NA
#z_dat

# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z_dat,
  # Detection/nondetection data
  det_data = detections_only,
  # Date
  cov1 = scale(full_dat[,21:26]),
  # Veg Density - site level
  cov2 = scale(full_dat[,14]),
  # Background noise
  cov3 = scale(full_dat[,8:13]),
  # Effort 
  cov4 = scale(full_dat[,15:20]),
  n_unit = nrow(detections_only),
  n_rep = ncol(detections_only)
)

# Set initial values (optional)
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = rnorm(1, 0, 1),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}

# Run JAGS model
jags_fit_modifiedpriorinit <- jags(data = jags_data, 
                 model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_Occ_DetectionModel1_ModifiedDatePriorInit.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = 30000, 
                 n.burnin = 10000, 
                 n.chains = 3,
                 n.thin = 2,
                 parallel = TRUE,
                 inits = init_func)
#saveRDS(jags_fit_global, file = "./R_Scripts/7_Model_Scripts/JAGS_Global_Detection_Model.Rdata")
print(jags_fit_modifiedpriorinit)
tracedens_jags(jags_fit_modifiedpriorinit)

#Plot posterior for b1 against posterior for b3 and intercept - if it looks like a "track", the covariates are pulling on each other too much
plot(jags_fit_modifiedpriorinit$sims.list$b0 ~ jags_fit_modifiedpriorinit$sims.list$b3)
plot(jags_fit_modifiedpriorinit$sims.list$b0 ~ jags_fit_modifiedpriorinit$sims.list$b1)
plot(jags_fit_modifiedpriorinit$sims.list$b3 ~ jags_fit_modifiedpriorinit$sims.list$b1)
```
I'm still getting the error node inconsistent with parameters

What the issue was: I was reading in data that wasn't scaled, but my data was specified to pull values for NAs that were scaled, which created an odd distribution in the data for things like date, which has unscaled values in the hundreds. When the model was trying to multiply the effect of these covariates times the covariate value and relate that to a presence/absence value, it was multiplying it by very large (date) and very small (decibel) numbers. Therefore, when it was trying to transform to a logit scale, it would be very confident that a later date (*222 or something like that) would be a very high probability of presence, then unable to converge why it was a zero. 

```







```{R Here Lies my Failed Attempts and Stuff I Dont Need, include = FALSE, eval = FALSE}
#detections_only_1rem <- as.matrix(full_dat_1rem %>% select(matches("det")))

# Extract MCMC chains
#chains <- as.mcmc(jags_fit_global)

# global_chains <- extract(jags_fit_global)
# test$beta_det

# Just looking at veg_density
# ggplot(data = test, mapping = aes(x = Parameter, y = Values)) + 
#   geom_boxplot() 
# 
# jags_chains_global %>% ggplot(aes(x = ))

# Look at the proportion that is on the same size of zero as the mean (derived from the summary of the model)
# make a violin plot (package vioplot I think)
# If the percent of posterior samples that fall on the same of zero as the mean is >90%, you have reason to believe there's a strong effect (F value in jags)
# don't go down to 70%
# Effect of effort 
# nI <- length(test$beta_det[,4])
# length(which(test$beta_det[,4] < 0))/nI

# # Manually calculating f values
# nI <- length(jags_chains_global[,1])
# # Date
# ## Test if mean is positive or negative
# jags_fit_global$mean$b1
# # Negative, so we're testing which side of zero it falls on
# date_fval <- length(which(jags_chains_global$b1 < 0))/nI

# Running old JAGS model from Test GPT code - DO NOT USE OVERLY COMPLICATED
full_dat <- readRDS("./Data/Habitat_Model_Covariates/Full_DetectionData_JAGSModels_HabChap.Rdata")
# b1/cov1: date
# b2/cov2: veg density
# b3/cov3: background noise
# b4/cov4: effort

# Create the jags_model
cat("
  model{
    # Loop through each sampling unit
    for(i in 1:n_unit){
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      for(j in 1:n_rep){
        det_data[i,j] ~  dbern(Z[i]*theta[i,j])  # Create detection data drawn from a bernoulli distribution incorporating the true presence (Z) with probability of detection theta
        logit(theta[i,j]) <- b0 + b1*cov1[i,j] + b2*cov2[i] + b3*cov3[i,j] + b4*cov4[i,j]
      }
    }
    # Imputation for missing covariates
    for(i in 1:n_unit){
      for(j in 1:n_rep){
        # Impute cov1
        cov1[i,j] <- ifelse(is_na_cov1[i,j] == 1, imputed_cov1[i,j], cov1[i,j])
        imputed_cov1[i,j] ~ dnorm(0, 1)
        # Impute cov3
        cov3[i,j] <- ifelse(is_na_cov3[i,j] == 1, imputed_cov3[i,j], cov3[i,j])
        imputed_cov3[i,j] ~ dnorm(0, 1)
        # Impute cov4
        cov4[i,j] <- ifelse(is_na_cov4[i,j] == 1, imputed_cov4[i,j], cov4[i,j])
        imputed_cov4[i,j] ~ dnorm(0, 1)
      }
      # Impute cov2
      cov2[i] <- ifelse(is_na_cov2[i] == 1, imputed_cov2[i], cov2[i])
      imputed_cov2[i] ~ dnorm(0, 1)
    }
    # Priors
    a0 ~ dnorm(0, 0.01)
    b0 ~ dnorm(0, 0.01)
    b1 ~ dunif(-2, 2)
    b2 ~ dnorm(0, 1)
    b3 ~ dnorm(0, 1)
    b4 ~ dnorm(0, 1)
  }
  ", file = "JAGS_Occ_DetectionModel2_Imputed.txt")

# Pull out the detections so that you have n_unit and n_site
detections_only <- full_dat[,2:7]
# Create a list of data for JAGS
# Date
cov1 <- full_dat[,21:26]
# Veg Density - site level
cov2 <- full_dat[,14]
# Background noise
cov3 <- full_dat[,8:13]
# Effort 
cov4 <- full_dat[,15:20]

# Create indicator matrices for missing values
is_na_cov1 <- is.na(cov1)
is_na_cov2 <- is.na(cov2)
is_na_cov3 <- is.na(cov3)
is_na_cov4 <- is.na(cov4)

# Convert logical matrices to numeric (0/1)
is_na_cov1 <- ifelse(is_na_cov1, 1, 0)
is_na_cov2 <- ifelse(is_na_cov2, 1, 0)
is_na_cov3 <- ifelse(is_na_cov3, 1, 0)
is_na_cov4 <- ifelse(is_na_cov4, 1, 0)

# Create a list of data for JAGS
jags_data <- list(
  det_data = detections_only,
  cov1 = cov1,
  cov2 = cov2,
  cov3 = cov3,
  cov4 = cov4,
  is_na_cov1 = is_na_cov1,
  is_na_cov2 = is_na_cov2,
  is_na_cov3 = is_na_cov3,
  is_na_cov4 = is_na_cov4,
  n_unit = nrow(detections_only),
  n_rep = ncol(detections_only)
)

# Set initial values (optional)
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = runif(1, min = -2, max = 2),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}

# Run JAGS model
jags_fit_global <- jags(data = jags_data, 
                 model.file = "JAGS_Occ_DetectionModel2_Imputed.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = 10000, 
                 n.burnin = 2000, 
                 n.chains = 3,
                 inits = init_func)

jags_fit_global
```
