---
title: "JAGS Detection Model 3"
author: "Anna Kurtin"
date: "`r Sys.Date()`"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jagsUI)
library(tidyverse)
library(jagshelper)
library(ggplot2)
library(coda)
library(janitor)
library(loo)
source("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/5_Visualization/Create_HexCodes_CuckooColorBlind.R")
source("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/6_Function_Scripts/Create_SiteColumn_fromPointID.R")
```

## Data Reading and Formatting

I'm going to read in the data, look at it's distribution, and format it for use in JAGS. As part of this formatting, I will be centering and scaling the data, so that each covariate has a mean of 0 and standard deviation of 1. This means that the covariates are interpretable compared to one another and allows us to pull from a normal distribution for our priors and initial values. 

Format detection data, which has been separated into two-week survey periods. This data represents a 1 if any BBCU vocalizations were confirmed at any point within the site within the two week survey period. 

*Scripts: ./R_Scripts/3_Data_Wrangling_Tidying/Detection_History/CreateDetectionHistoryPeriods_camtrapR.R & /Create_Datasheets_forDetectionHistory.R*

```{r Detection Data}
# y data: read in a dataframe that has a row for every point and a column for every detection period
detections_orig <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Detection_History/2023_All_ARUs/Outputs/DetectionHist_CamTrapR/bbcu__detection_history__with_effort__14_days_per_occasion__2024-04-29.csv") %>% clean_names
survey_startdate <- colnames(detections_orig)
# Make a copy of the data
detections <- detections_orig
colnames(detections) <- c("site_session","det_survey1","det_survey2","det_survey3","det_survey4","det_survey5","det_survey6")
# Split apart the detections into site and point
detections <- detections %>% separate(site_session, into = c("site_id","session_name"), sep = "__")

```

Format the date covariate. This is a survey-level covariate that represents the start date of each survey period. We can see from the distribution that this follows a uniform distribution. 

```{R Date, out.width = "50%", out.height = "50%"}
# Create the values for date
# Remove the value with just "x", remove "x" from the rest, reformat dates
surveys_cleaned <- survey_startdate[-1] %>% 
  gsub("^x", "", .) %>%            # Remove the leading "x"
  gsub("_", "-", .)                # Replace underscores with hyphens
# then convert it into a dataframe
# Convert to a dataframe
surveys_df <- data.frame(survey_period = surveys_cleaned)
# Split the survey_period into start_date and end_date 
surveys_df <- surveys_df %>%
  separate(survey_period, into = c("start_date", "end_date"),sep = "(?<=\\d{4}-\\d{2}-\\d{2})-")
# Convert it to date
surveys_df <- surveys_df %>% mutate(start_date = as.Date(start_date, format = "%Y-%m-%d"),
                                    end_date = as.Date(end_date, format = "%Y-%m-%d"))
# Convert to Julian Date
surveys_df <- surveys_df %>% mutate(start_julian = yday(start_date),
                                        end_julian = yday(end_date))
# Scale the start date
surveys_df$startdate_scaled <- scale(surveys_df$start_julian, center = TRUE)[,1]
# Pivot wider then rename the columns
start_dates <- surveys_df$startdate_scaled
# Plot the distribution
hist(surveys_df$startdate_scaled, col = cuckoo_palette[1], main = "Distribution of Scaled Date Covariate")
```

Format the covariate for vegetation density. This is a composite index of vegetation density at the site derived from the total shrub cover and total number of trees present within the plot. For each site, I took the total percent shrub cover and standardized it to the maximum percent shrub cover across all sites. Then, I took the total number of trees present and standardized it to the maximum total number of trees across all sites. Then I summed these two values to create a composite vegetation density index. This covariate follows a normal distribution. 

*Script: ./R_Scripts/3_Data_Wrangling_Tidying/Detection_Covariates/Combine_Veg_Survey_Plant_Data.R*

```{R Vegetation Density, out.width = "50%", out.height = "50%"}
# Site-level covariates
# Read in vegetation density for each site
vegdense <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Detection_Covariates/ShrubTreeDensityComposite_2023_ARUPoints.csv")
vegdense <- vegdense %>% create_site_col()
# take the average of this across the site
vegdense_avg <- vegdense %>% group_by(site_id,sampling_design) %>% summarize(veg_density_avg = round(mean(composite_dense,na.rm = TRUE),2))
vegdense_avg <- as.data.frame(vegdense_avg)
# Scale this covariate
vegdense_avg$veg_density_avg_scaled <- scale(vegdense_avg$veg_density_avg)[,1]
vegdense_fin <- vegdense_avg %>% select(site_id,veg_density_avg_scaled)
# Look at the distribution 
hist(vegdense_avg$veg_density_avg_scaled, col = cuckoo_palette[1], main = "Distribution of Scaled Veg Density Covariate")
```

Format the background noise covariate. This covariate represents the average background noise in decibels during each survey period at the site. To create this covariate, I calculated the average decibels within the bandpass filter used by the cuckoo classifier (200-3500 Hz) for each audio file. Then, I assigned each audio file to a survey period, based on the same date intervals used for detection history. I grouped by site ID and survey period, then calculated the average decibel value. This covariate follows a normal distribution. 

*Script: ./R_Scripts/3_Data_Wrangling_Tidying/Detection_Covariates/Create_Survey_Level_BackgrounddB.R*

```{R Background Noise, out.width = "50%", out.height = "50%"}
# background noise
background_db <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Detection_Covariates/BackgroundNoiseBandpass_2023_ARUPoints.csv")
colnames(background_db) <- c("site_id","backdb_survey1","backdb_survey2","backdb_survey3","backdb_survey4","backdb_survey5","backdb_survey6")
# Pivot longer for scaling
db_long <- background_db %>% pivot_longer(cols = c(backdb_survey1,backdb_survey2,backdb_survey3,backdb_survey4,backdb_survey5,backdb_survey6), names_to = "survey_period",values_to = "db" )
# Scale it
db_long$db_scaled <- scale(db_long$db, center = TRUE)[,1]
# Check the distribution
hist(db_long$db_scaled, xlab = "Background dB", main = "Distribution of Scaled Background Noise", col = cuckoo_palette[1])
# Pivot wider for use in model
db_fin <- db_long %>% select(-db) %>% pivot_wider(names_from = survey_period, values_from = db_scaled)
```

Format the effort data. This data was created along with the two-week detection periods. I created an effort covariate by extracting the size of each audio file and writing that to a datasheet. Then, I counted a unit as active only if it had an audio file from a given day and if that audio file met the correct size threshold. I screened out audio files that were the proper size but had issues within the recording so that you could not detect sounds in the range of cuckoo vocalizations (200-3500 Hz). With this record of audio files recorded per day for each recording unit, I then used package camtrapR to calculate effort to represent the hours of audio recorded per day/percentage of the 2 hours per day successfully recorded. In camtrapR, I grouped recording units by site and summed up the effort per site across each survey period.  

*Scripts: ./R_Scripts/3_Data_Wrangling_Tidying/Detection_History/CreateDetectionHistoryPeriods_camtrapR.R & /Create_Datasheets_forDetectionHistory.R*

```{R Effort, out.width = "50%", out.height = "50%"}
# Read in detection and effort data 
effort <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Detection_History/2023_All_ARUs/Outputs/DetectionHist_CamTrapR/bbcu__effort__not_scaled_14_days_per_occasion__2024-04-29.csv") %>% clean_names
#survey_periods <- colnames(effort)
# Rename columns
colnames(effort) <- c("site_session","effort_survey1","effort_survey2","effort_survey3","effort_survey4","effort_survey5","effort_survey6")
# Clean up the columns
effort <- effort %>% separate(site_session, into = c("site_id","session_name"), sep = "__") %>% select(-session_name)
# Pivot longer for scaling
effort_long <- effort %>% pivot_longer(cols = c(effort_survey1,effort_survey2,effort_survey3,effort_survey4,effort_survey5,effort_survey6), names_to = "survey_period",values_to = "effort" )
# Sclae it
effort_long$effort_scaled <- scale(effort_long$effort, center = TRUE)[,1]
# Look at the distribution
hist(effort_long$effort_scaled, xlab = "Effort", main = "Distribution of Scaled Survey Effort", col = cuckoo_palette[1])
# Pivot wider for use in model 
# Pivot wider for use in model
effort_fin <- effort_long %>% select(-effort) %>% pivot_wider(names_from = survey_period, values_from = effort_scaled)
```

After all of my covariates were formatted and scaled, I combined them into one dataframe to use in my model. 
```{R Combine Data, eval = FALSE}
# Combine them
full_dat <- left_join(detections,db_fin, by = "site_id")
full_dat <- left_join(full_dat,vegdense_fin, by = "site_id")
full_dat <- left_join(full_dat, effort_fin, by = "site_id")
full_dat <- full_dat %>% mutate(start_date_s1 = start_dates[1],
                     start_date_s2 = start_dates[2],
                     start_date_s3 = start_dates[3],
                     start_date_s4 = start_dates[4],
                     start_date_s5 = start_dates[5],
                     start_date_s6 = start_dates[6])
# Remove unwanted column
full_dat <- full_dat %>% select(-c(session_name))
# Save for future use
#saveRDS(full_dat, file ="./Data/Habitat_Model_Covariates/Full_DetectionDataSCALED_JAGSModels_HabChap.Rdata")
```

Let's load in the data that I just formatted and scaled. 

```{r Load in Data}
full_dat <- readRDS(file ="C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/Full_DetectionDataSCALED_JAGSModels_HabChap.Rdata")
```

## Quadratic effect for date

```{R Set Up Quadratic Model, eval = FALSE}
# b1/cov1: date
# b2/cov2: veg density
# b3/cov3: background noise
# b4/cov4: effort

# Create the jags_model
cat("
  model{
    # Loop through each sampling unit
    for(i in 1:n_unit){
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      
      cov2[i] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
      
      for(j in 1:n_rep){
        cov1[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov3[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov4[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        
        det_data[i,j] ~  dbern(Z[i]*theta[i,j])  # Create detection data drawn from a bernoulli distribution incorporating the true presence (Z) with probability of detection theta
        logit(theta[i,j]) <- b0 + b1*cov1[i,j]^2 + b2*cov2[i] + b3*cov3[i,j] + b4*cov4[i,j]
        
      }
    }
    
    # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dnorm(0, 1)
    b2 ~ dnorm(0, 1)
    b3 ~ dnorm(0, 1)
    b4 ~ dnorm(0, 1)
  }
  ", file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_HabDetMod3_QuadDate.txt")
```


```{R Specify Data for Model, eval = FALSE}
# Pull out the detections so that you have n_unit and n_site
detections_only <- full_dat[,2:7]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat <- rowSums(detections_only, na.rm = T)
z_dat[z_dat > 1] <- 1
z_dat[z_dat == 0] <- NA

# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z_dat,
  # Detection/nondetection data
  det_data = detections_only,
  # Date
  cov1 = full_dat[,21:26],
  # Veg Density - site level
  cov2 = full_dat[,14],
  # Background noise
  cov3 = full_dat[,8:13],
  # Effort 
  cov4 = full_dat[,15:20],
  n_unit = nrow(detections_only),
  n_rep = ncol(detections_only)
)

# Set initial values (optional)
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = rnorm(1, 0, 1),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}
```


```{R Run Quadratic Model, eval = FALSE}
# Run JAGS model
habdet_m3 <- jags(data = jags_data, 
                 model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_HabDetMod3_QuadDate.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = 20000, 
                 n.burnin = 4000, 
                 n.chains = 3,
                 n.thin = 2,
                 parallel = TRUE,
                 inits = init_func)
saveRDS(habdet_m3, file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Actual_Data/JAGS_Global_Detection_Model_QuadDate.Rdata")
```

```{R Evaluating Convergence QuadMod}
# Read in the model that we saved after running the above code
habdet_m3 <- readRDS("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Actual_Data/JAGS_Global_Detection_Model_QuadDate.Rdata")

# Evaluating convergence
print(habdet_m3)
tracedens_jags(habdet_m3, parmfrow = c(3,3))
```

```{R Create Violin Plots Quad Mod}
# Plot posterior distribution
chains_m3 <- jags_df(habdet_m3)
# Select the chains for our covariates of interest
chains_viol3 <- chains_m3 %>% select(b1,b2,b3,b4)
# Rename them to be more interpretable
colnames(chains_viol3) <- c("date","veg_density","background_noise","effort")
# Pivot this longer so that we can visualize it
chains_viol_long3 <- chains_viol3 %>% pivot_longer(cols = c("date","veg_density","background_noise","effort"),names_to = "Parameter", values_to = "Values")

# Create the violin plot
ggplot() +
  geom_violin(data=chains_viol_long3, aes(x = Parameter, y = Values,fill = Parameter),trim = FALSE) +
  labs(title = "Posterior Distribution of Detection Covariates") +
  scale_fill_manual(values = c("date"=palette_5[1], "background_noise"=palette_5[2], "effort" = palette_5[3], "veg_density" = palette_5[4])) + 
  geom_hline(yintercept = 0, linetype = "solid", color = "black") +
  theme_minimal()
```
What changed?

* base probability of detection is lower (58% with this model versis 70% with the initial model)

* weaker effect of date

* 

How do I know which model fits better?

* Conduct a posterior predictive check 

* look at Rsquared or root mean r squared

```{R Posterior Predictive Checks}
#test <- as.data.frame(habdet_m3$sims.list)
# Just using chains m3 instead since I already made that
loglikemat_3 <- as.matrix(chains_m3)
loo(loglikemat_3)
```