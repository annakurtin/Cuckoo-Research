---
title: "Habitat Chapter Detection Sub Model Final 4"
author: "Anna Kurtin"
date: "10-25/2024"
output: html_document
---

### Explanation of model: 

This model is me redoing the detection submodel to streamline and clarify my covariate selection process for the final global model. Here is a list of  the previous iterations of this model (also on OneNote):

* Detection Submodel 1: Naive state model, detection model with veg density, background decibels, survey effort, survey start date. Nothing taken from this model, I did this wrong and had the wrong idea about priors. 

* Detection Submodel 2: Naive state model, detection model with veg density, background decibels, survey effort, survey start date. Used different priors (normal). All covariates included, no quadratic effects.

* Detection Submodel 3: No markdown from this iteration, skipped

* Detection Submodel 4: Same model structure as submodel 3: naive state model, detection model with veg density, background decibels, survey effort, survey start date, used proper logistic priors

* Detection Submodel 5: Same model structure and priors as submodel 4 but with quadratic effect on date

* Detection Submodel 6: Same model structure and priors as submodel 5 but with veg density (non significant effect) removed. 

* Detection Submodel Final 2: Same model as submodel 5 with updated cleaned data

* Detection Submodel Final 3: model without quadratic effect on date and instead including quadratic effect on background db. Based on waic was not supported, not used

* This model: same structure as submodel final 2, adding in a covariate for ARU type (0 for AM only, 1 for SongMeter present)



### Model structure:

**State Model:** naive

$$
Z_i \sim Bernoulli(\psi_i)
$$


**Detection Model**: with 5 covariates

$$
y_{ij} \sim Bernoulli(p_{ij}| \beta_1, \beta_2, \beta_3, \beta_4, \beta_{4Q}, \beta_{5})
$$

$$\beta_1: \text{vegetation density}$$ 

$$\beta_2: \text{background decibels}$$ 

$$\beta_3: \text{survey effort}$$ 

$$\beta_4: \text{survey start date}$$  

$$\beta_4Q: \text{quadratic effect of start date}$$ 

$$\beta_5: \text{aru type present at site}$$



**Handling of NAs:** imputation for missing habitat parameters, skipping missing detection data


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(jagsUI)
library(rjags)
library(tidyverse)
library(jagshelper)
library(ggplot2)
library(patchwork)
library(ggdist)
library(beepr)
source("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/5_Visualization/Create_HexCodes_CuckooColorBlind.R")
```
 

```{R}
# Load in most recent version of data
full_dat <- read.csv("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/Data/Habitat_Model_Covariates/HabChap_DetOccCovsFull_SCALED_10-25.csv")
```

### Visualize distribution of scaled covariates

```{R Visual Cov 1 Veg Dense}
veg <- full_dat %>% select(2:7, veg_density_avg_scaled)
veg$det <- apply(veg[,1:6], 1, max, na.rm = TRUE)

vegplot <- ggplot(veg) +
  geom_jitter(aes(x = veg_density_avg_scaled, y = det), color = palette_5[1], width = 0, height = .05) +
  geom_smooth(aes(x = veg_density_avg_scaled, y = det), color = palette_5[1],se = FALSE, method = "glm", method.args = list(family = "binomial")) +
  labs(x = "Vegetation Density", y = "Detection") + theme_minimal()
```


```{R Visual Cov 6 ARU Type}
aru <- full_dat %>% select(2:7, SM_present)
aru$det <- apply(aru[,1:6], 1, max, na.rm = TRUE)

aruplot <- ggplot(aru) +
  geom_jitter(aes(x = SM_present, y = det), color = palette_8[1], width = 0.05, height = .05) +
  geom_smooth(aes(x = SM_present, y = det), color = palette_8[1],se = FALSE, method = "glm", method.args = list(family = "binomial")) +
  labs(x = "SongMeter Used at Site", y = "Detection") + theme_minimal()
```

```{R Pivot Survey Level Data}
#survey_dat <- full_dat %>% select(2:7, 24:29,31:42)
# first select the data columns of interest
dates <- full_dat %>% select(site_id, 39:44)
effort <- full_dat %>% select(site_id, 33:38)
backdb <- full_dat %>% select(site_id, 26:31)

# Then for each sub dataset, pivot longer, rename to s1,s2, etc and unite with site_id for site_survey
dates_long <- dates %>% pivot_longer(cols = c(start_date_s1,
                                               start_date_s2,
                                               start_date_s3,
                                               start_date_s4,
                                               start_date_s5,
                                               start_date_s6), names_to = "survey_period",values_to = "date") 
dates_long <- dates_long %>% mutate(survey_period = case_when(survey_period == "start_date_s1" ~ "s1",
                                                 survey_period == "start_date_s2" ~ "s2",
                                                 survey_period == "start_date_s3" ~ "s3",
                                                 survey_period == "start_date_s4" ~ "s4",
                                                 survey_period == "start_date_s5" ~ "s5",
                                                 survey_period == "start_date_s6" ~ "s6")) 
dates_long <- dates_long %>% unite(site_survey, c("site_id", "survey_period"), sep = "_")

effort_long <- effort %>% pivot_longer(cols = c(effort_survey1,
                                               effort_survey2,
                                               effort_survey3,
                                               effort_survey4,
                                               effort_survey5,
                                               effort_survey6), names_to = "survey_period",values_to = "effort") 
effort_long <- effort_long %>% mutate(survey_period = case_when(survey_period == "effort_survey1" ~ "s1",
                                                 survey_period == "effort_survey2" ~ "s2",
                                                 survey_period == "effort_survey3" ~ "s3",
                                                 survey_period == "effort_survey4" ~ "s4",
                                                 survey_period == "effort_survey5" ~ "s5",
                                                 survey_period == "effort_survey6" ~ "s6")) 
effort_long <- effort_long %>% unite(site_survey, c("site_id", "survey_period"), sep = "_")


backdb_long <- backdb %>% pivot_longer(cols = c(backdb_survey1,
                                               backdb_survey2,
                                               backdb_survey3,
                                               backdb_survey4,
                                               backdb_survey5,
                                               backdb_survey6), names_to = "survey_period",values_to = "backdb") 
backdb_long <- backdb_long %>% mutate(survey_period = case_when(survey_period == "backdb_survey1" ~ "s1",
                                                 survey_period == "backdb_survey2" ~ "s2",
                                                 survey_period == "backdb_survey3" ~ "s3",
                                                 survey_period == "backdb_survey4" ~ "s4",
                                                 survey_period == "backdb_survey5" ~ "s5",
                                                 survey_period == "backdb_survey6" ~ "s6")) 
backdb_long <- backdb_long %>% unite(site_survey, c("site_id", "survey_period"), sep = "_")

# Format detection data long as well 
dets <- full_dat %>% select(1:7)
det_long <- dets %>% pivot_longer(cols = c(det_s1,
                                               det_s2,
                                               det_s3,
                                               det_s4,
                                               det_s5,
                                               det_s6), names_to = "survey_period", values_to = "det")
det_long <- det_long %>% mutate(survey_period = case_when(survey_period == "det_s1" ~ "s1",
                                                 survey_period == "det_s2" ~ "s2",
                                                 survey_period == "det_s3" ~ "s3",
                                                 survey_period == "det_s4" ~ "s4",
                                                 survey_period == "det_s5" ~ "s5",
                                                 survey_period == "det_s6" ~ "s6")) 
det_long <- det_long %>% unite(site_survey, c("site_id", "survey_period"), sep = "_")

# Unite all of them into one dataframe 
dat_long <- det_long %>% left_join(dates_long, by = "site_survey") %>% left_join(effort_long, by = "site_survey") %>% left_join(backdb_long, by = "site_survey")
```

```{R Visual Cov 2 Back dB}
dbplot <- ggplot(dat_long) +
  geom_jitter(aes(x = backdb, y = det), color = palette_5[2], width = 0, height = .05) +
  geom_smooth(aes(x = backdb, y = det), color = palette_5[2], se = FALSE, method = "glm", method.args = list(family = "binomial")) +
  labs(x = "Background Noise", y = "Detection") + theme_minimal()
```

```{R Visual Cov 3 Survey Effort}
effortplot <- ggplot(dat_long) +
  geom_jitter(aes(x = effort, y = det), color = palette_5[3], width = .05, height = .05) +
  geom_smooth(aes(x = effort, y = det), color = palette_5[3], se = FALSE, method = "glm", method.args = list(family = "binomial")) +
  labs(x = "Survey Effort", y = "Detection") + theme_minimal()
# What's going on with this????
```

```{R Visual Cov 4 Date}
dateplot <- ggplot(dat_long) +
  geom_jitter(aes(x = date, y = det), color = palette_5[4], width = .05, height = .05) +
  geom_smooth(aes(x = date, y = det), color = palette_5[5], se = FALSE, method = "glm", method.args = list(family = "binomial")) +
  labs(x = "Date", y = "Detection") + theme_minimal()
```

```{R Arrange Figures, message = FALSE}
vegplot + aruplot

effortplot + dateplot + dbplot

```

### Set Up Detection Submodel:

```{R Set up Data}
detections <- full_dat[,2:7]

# Have to specify the z data for a true presence if there was a 1 at any time 
z_dat <- rowSums(detections, na.rm = T)
z_dat[z_dat > 1] <- 1
z_dat[z_dat == 0] <- NA

# get the length of non-nas in the detection data
get_na <- function(x){(max(which(!is.na(x))))}
# Apply this to our data
miss <- apply(detections, 1, get_na)
```



```{R}
# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z_dat,
  # Detection/nondetection data
  det_data = detections,
  # Veg density
  cov1 = veg$veg_density_avg_scaled,
  # Background noise
  cov2 = backdb[,-1],
  # Effort 
  cov3 = effort[,-1],
  # Date
  cov4 = dates[,-1],
  # SongMeter used
  cov5 = aru$SM_present,
  n_unit = nrow(detections),
  miss = miss
)

# Set initial values \
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = rnorm(1, 0, 1),
    b2 = rnorm(1, 0 ,1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1),
    b4Q = rnorm(1, 0, 1),
    b5 = rnorm(1, 0, 1)
  )
}

```


```{R, echo = TRUE, eval = FALSE}
cat("
  model{
        # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dlogis(0, 1)
    b2 ~ dlogis(0, 1)
    b3 ~ dlogis(0, 1)
    b4 ~ dlogis(0, 1)
    b4Q ~ dlogis(0, 1)
    b5 ~ dlogis(0, 1)
  
    # Likelihood
    # Loop through each sampling unit
    for(i in 1:n_unit){
      # Impute missing habitat data
      cov1[i] ~ dnorm(0, 1)
  
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0 
      
      for(j in 1:miss[i]){
       # Impute missing detection data
        cov2[i,j] ~ dnorm(0, 1)
        cov3[i,j] ~ dnorm(0, 1)
        cov4[i,j] ~ dnorm(0, 1)
  
        det_data[i,j] ~  dbern(mu_p[i,j])  # Create probability density drawn for each site
        mu_p[i,j] <- Z[i]*theta[i,j]
        logit(theta[i,j]) <- p[i,j]
        p[i,j] <- b0 + b1*cov1[i] + b2*cov2[i,j] + b3*cov3[i,j] + b4*cov4[i,j] + b4Q*(cov4[i,j]^2) + b5*cov5[i]
  
        # Create detection data without NAs
        det_na[i,j] <- det_data[i,j]
         
        #Calculate Bayes p-value - plug the parameters into the equation for each sample of the posterior and use this to create new observations
        Presi[i,j] <- (det_na[i,j] - theta[i,j])^2 # Calculate squared residual of observed data
        y_new[i,j] ~ dbern(mu_p[i,j])             # Simulate observed data
        Presi_new[i,j] <- (y_new[i,j] - theta[i,j])^2 # Calculate squared residual error of simulated data
        
      }
    }
    
    # Derived parameters
    ## Calculate sum of squared residual errors for observed 
    for(i in 1:n_unit){
       sobs[i] <- sum(Presi[i,1:miss[i]])
       ssim[i] <- sum(Presi_new[i,1:miss[i]])
    }
    
    # Use SSEs to calculate Bayesian p-values
    SSEobs <- sum(sobs[])
    SSEsim <- sum(ssim[])
    p_val <- step(SSEsim - SSEobs)
    
  }
  ", file = "C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_DetMod_Fin4.txt")
```

```{R Run Model, results = FALSE}
habdet <- jags(data = jags_data, 
                 model.file = "C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_DetMod_Fin4.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4","b4Q", "b5", "p_val", "SSEobs","SSEsim"),
                 n.iter = 20000, 
                 n.burnin = 10000, 
                 n.chains = 3,
                 n.thin = 2,
                 inits = init_func)
beep()
```

```{R save model, eval = FALSE}
saveRDS(habdet,"C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Actual_Data/DetSubModel_Final4.Rdata")
habdet <- readRDS("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Actual_Data/DetSubModel_Final4.Rdata")
```

### Examine Outputs and Model Convergence: 

$$\beta_1: \text{vegetation density}$$ 

$$\beta_2: \text{background decibels}$$ 

$$\beta_3: \text{survey effort}$$ 

$$\beta_4: \text{survey start date}$$  

$$\beta_{4Q}: \text{quadratic effect of start date}$$ 

$$\beta_5: \text{ARU type}$$  

```{R}
summary(habdet)
tracedens_jags(habdet, parmfrow = c(2,3))
```

### Examine Model Fit:

```{R Make Inference on P Value}
plot(habdet$sims.list$SSEobs,habdet$sims.list$SSEsim,xlab="SSEobs",ylab="SSEsim")
abline(0,1,lwd = 2, col = "red")
print(paste0("P value is:",round(habdet$mean$p_val, 2)))
```

### Median Effect Sizes: 

```{R Transform Parameter Estimates}
# Transform median values from posterior distribution into probability 
psi <- round(plogis(habdet$q50$a0), 3)
print(paste0("psi:", psi))

p_hat <- round(plogis(habdet$q50$b0), 3)
print(paste0("p hat:", p_hat))

b1 <- round(plogis(habdet$q50$b1), 3)
print(paste0("veg density:", b1))

b2 <- round(plogis(habdet$q50$b2), 3)
print(paste0("background noise:", b2))

b3 <- round(plogis(habdet$q50$b3), 3)
print(paste0("survey effort:", b3))

b4 <- round(plogis(habdet$q50$b4), 3)
print(paste0("date:", b4))

b4Q <- round(plogis(habdet$q50$b4Q), 3)
print(paste0("quadratic effect of date:", b4Q))

b5 <- round(plogis(habdet$q50$b5), 3)
print(paste0("aru type:", b5))
```


```{R Cumulative Detection Probability}
p <- round(plogis(habdet$q50$b0),4)
print(paste0("Estimate for detection probability of a single ARU survey:",p))

n <- 6
p_star <- round(1 - (1 - p)^n, 4)
print(paste0("Estimate for cumulative ARU detection probability:",p_star))
```


### Plot Results and Interpret Output

```{R Create Plots of Posterior Distribution}
# plotting with ggdist: https://mjskay.github.io/ggdist/
# Plot posterior distribution
chains <- jags_df(habdet)
# Select he chains for our covariates of interest
chains_viol <- chains %>% select(b1,b2,b3,b4,b4Q, b5)
# Rename them to be more interpretable
colnames(chains_viol) <- c("veg density",
                           "background noise",
                           "effort",
                           "date", 
                           "quadratic date",
                           "aru type")
# Pivot this longer so that we can visualize it
chains_viol_long <- chains_viol %>% pivot_longer(cols = c("veg density",
                                                          "background noise",
                                                          "effort",
                                                          "date", 
                                                          "quadratic date",
                                                          "aru type"),names_to = "parameter", values_to = "values")

f_stat <- data.frame(
  parameter = c("veg density",
                "background noise",
                "effort","date", 
                "quadratic date",
                "aru type"),
  median_value = c(paste0("F: ", round(habdet$f$b1,2)), 
                   paste0("F: ",round(habdet$f$b2,2)), 
                   paste0("F: ",round(habdet$f$b3,2)), 
                   paste0("F: ",round(habdet$f$b4,2)), 
                   paste0("F: ",round(habdet$f$b4Q,2)),
                   paste0("F: ",round(habdet$f$b5,2)))
)

# Create the slab interval plot
ggplot(data = chains_viol_long, aes(x = values, y = parameter, fill = parameter)) + 
  # Plot posterior distributions
  stat_slabinterval() +
  # Establish colors
  scale_fill_manual(values = c("veg density"=palette_5[1], 
                               "background noise" = palette_5[2],
                               "effort"=palette_5[3], 
                               "date" = palette_5[4], 
                               "quadratic date" = palette_5[5],
                               "aru type" = palette_8[1])) +
  # Add a line for 0 to show overlap of posterior
  geom_vline(xintercept = 0) +
  # Remove background color from plots
  theme_minimal() +
  # Turn off the legend
  guides(fill = FALSE)+
  # Add median values as text labels
  geom_text(data = f_stat, aes(x = 3.5, y = parameter, label = median_value), color = "black", hjust = .6) 
# How can I add the prior distributions on top of this?
```



### WAIC: compare with other models

I'm extracting the WAIC for this model to compare it to versions of the detection model that don't include ARU type as a covariat (Detection Model Final 2).

```{R Extract WAIC}
# Extract samples from the posterior
f4_s <- jags.samples(habdet$model,
                           c("WAIC","deviance"),
                           type = "mean",
                           n.iter = 5000,
                           n.burnin = 1000,
                           n.thin = 2)
beep()
# Duplicate WAIC samples - the WAIC is the posterior mean for that sample?
f4_s$p_waic <- f4_s$WAIC
# Calculate a new waic by adding the waic plus the deviance
f4_s$waic_calc <- f4_s$deviance + f4_s$p_waic
# Create a new vector of the sum across each array (sum WAIC, sum deviance, sum p_waic, and sum waic_calc)
tmp <- sapply(f4_s, sum)
# Pull out the sum of the WAIC we calculated and the p_waic we duplicated
waic_f4 <- round(c(waic = tmp[["waic_calc"]], p_waic = tmp[["p_waic"]]),1)
# Calculate the standard error for WAIC
f4_waic_se <- sd(f4_s$waic_calc)/(sqrt(length(f4_s$waic_calc)))

waic_f4
f4_waic_se
```


### Interpreting outputs:

**Strongly significant covariates (CI doesn't overlap 0):**

* Date: 0.136 – shifts the “peak” of the detections across dates more positive/later in the season/slightly after the average recording date and affects the curve of the graph 

* Quadratic effect of date: 0.162 – makes a steeper slope on either end of the graph – detection probability drops off sooner

**Weakly significant covariates (F > .70):**

* Vegetation density (relative number of trees combined with relative density of shrubs > 1m tall - unitless composite metric): There is a 70% change that the true effect of vegetation density at a site on probability of detecting a BBCU given that it is present at a site is positive. The median estimate for the effect of vegetation density on detection probability is 0.532, which means that for every 1 sd increase in vegetation density above the mean (.23 increase above mean of .27), the probability of detecting a cuckoo increases by 53.2%

* Effort:	Effect size of effort: The median estimate for the effect of survey effort on detection probability is 0.533, which means that at the average value of the other covariates, for every 1 sd increase in survey effort above the mean (10 increase above 17) the probability of detecting C. erythropthalmus given they are present increases by 53.3%.

* Background noise: There is an 80% chance that the true effect of the average decibels in the band of cuckoo calling (I think it's 200  -3500?) is negative. The median estimate for the effect of background noise on detection probability is 0.451, which means for every 1 sd increase in background noise above the mean (4 dB increase above mean of -61 dB), the probability of detecting a BBCU given they are present decreases by 45.1%. 

**Unsuppported covariates (F < 0.70)**

* ARU type (1 for SongMeter present, 0 for only AudioMoths used): There is a 58% chance that the true effect of SongMeter presence on detection at a site is negative. This is not strong enough evidence to say that there is an effect. 