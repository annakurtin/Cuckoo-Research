---
title: "Recover Params JAGS Detection Model1"
author: "Anna Kurtin"
date: "`r Sys.Date()`"
output: html_document
---


```{r setup, include=FALSE}
#Anna's note: I copied over the original markdown (labeled with model 1) to include the fixed model that Thomas and I created.
knitr::opts_chunk$set(echo = TRUE)
library(jagsUI)
library(tidyverse)
library(jagshelper)
library(ggplot2)
source("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/5_Visualization/Create_HexCodes_CuckooColorBlind.R")
set.seed(123)
```

## Generate Data

In order to determine if my model will perform well, I will first be simulating data using specified values for probability of occupancy and probability of detection. Then I will run my detection sub model on that data to see if it can recover the correct parameters. 

I am simulating this data with a 20% probability of occupancy (based on naive estimates from our data) and a probability of detection of 60% (fairly high, so that we can test our model performance). 

```{R generate data}
# b1/cov1: date - uniform, survey level
# b2/cov2: veg density - normal, site level
# b3/cov3: background noise - normal, survey level
# b4/cov4: effort - normal - survey level 

nP <- 107  # number of points
nV <- 6    # number of repeated visits
nC <- 4    # number of covariates
#survey_covs <- 3

# create latent state (z) and detection (y) data storage
z <- NULL        # point-level latent state
y <- array(NA, dim = c(nP, nV))   # point-level observation data

# create separate arrays for each covariate and fill with randomly generated data
date_cov <- array(NA, dim = c(nP,nV))
dates <- runif(nV, -2,2) # Pull six (standardized) values for date
for (d in 1:nV){
  date_cov[,d] <- dates[d]
}
hist(date_cov, col = cuckoo_palette[1], main = "Distribution of Scaled Date")

# Fill in veg density
veg_cov <- rnorm(nP, 0, 1)
hist(veg_cov, col = cuckoo_palette[1], main = "Distribution of Simulated Scaled Veg Density")

# Fill in background noise
db_cov <- array(NA, dim = c(nP,nV))
for (i in 1:nP){
  for (j in 1:nV){
    db_cov[i,j] <- rnorm(1, 0, 1)
  }
}
hist(db_cov, col = cuckoo_palette[1], main = "Distribution of Simulated Scaled Background Noise")

# Fill in effort
effort_cov <- array(NA, dim = c(nP,nV))
for (i in 1:nP){
  for (j in 1:nV){
    effort_cov[i,j] <- rnorm(1, 0, 1)
  }
}
hist(effort_cov, col = cuckoo_palette[1], main = "Distribution of Simulated Scaled Effort")

# covariate effects (pull randomly for each covariate)
beta <- rnorm(nC, 0, 1)

psi0 <- qlogis(0.2) # probability of presence
p0 <- qlogis(0.6) # probability of detection

psi <- NULL
for (i in 1:nP){
  psi[i] = plogis(psi0)
}
# Assume equal probability of presence at all sites with this simulation

# get your true presence/non presence values from psi
z <- rbinom(nP, 1, psi)

# total number of detections across number of visits (nV)
y <- matrix(NA, nP, nV)
# fill in the detection matrix dependent on whether the animal was there (z) and whether it was detected (p0)
# add covariate effects here
for (i in 1:nP){
  for (j in 1:nV){
    y[i,j] <- rbinom(1, z[i], plogis(p0 + (beta[1]*date_cov[i,j]) + (beta[2]*veg_cov[i]) + (beta[3]*db_cov[i,j]) + (beta[4]*effort_cov[i,j])))
  }
}

```

## Set up model 

Here is the model parameterization. This is the exact same as the model I run on my actual data, I've just included it here for reference. 

```{R Set Up Model,eval = FALSE}
cat("
  model{
    # Loop through each sampling unit
    for(i in 1:n_unit){
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      
      cov2[i] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
      
      for(j in 1:n_rep){
        cov1[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov3[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        cov4[i,j] ~ dnorm(0, 1) # Assuming normal distribution for missing covariates
        
        det_data[i,j] ~  dbern(Z[i]*theta[i,j])  # Create detection data drawn from a bernoulli distribution incorporating the true presence (Z) with probability of detection theta
        logit(theta[i,j]) <- b0 + b1*cov1[i,j] + b2*cov2[i] + b3*cov3[i,j] + b4*cov4[i,j]
        
      }
    }
    
    # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dnorm(0, 1)
    b2 ~ dnorm(0, 1)
    b3 ~ dnorm(0, 1)
    b4 ~ dnorm(0, 1)
  }
  ", file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/Test_JAGS_Occ_DetectionModel2.txt")
```

Next, I establish the parameters for running the model itself. I specify first the number of chains, which is the number of times that JAGS will create independent posterior distributions. The use of multiple chains is to ensure that they arrive to the same conclusion - they are "evenly mixed" as we will see in the traceplots. Next, I specify the total number of iterations. This is the number of times the MCMC sampler will run and draw new posterior values. Next, I specify the burnin period, which is the number of iterations JAGS will "throw out" before keeping the posterior draws in the chain. This is becuase we randomize our starting/initial values for the each covariate, so if one of these initial values is very far off, the MCMC sampler will have some time to get it into a more normal range before we actually start monitoring the chains. Finally, I specify the number of thinning. This means that the MCMC sampler will keep out every other value it draws from the posterior distribution. Thinning the chains in this way reduces the amount of correlation between values in the posterior distribution and increases their independence. 

```{R Establish Parameters for Running Models}
# sampler settings that I use in my main model
nc <- 3 # number of chains
ni <- 20000 # number of iterations
nb <- 4000 # number burnin
nt <- 2 # number of thinning
```

Next, I package the data together for JAGS to use. I also specify the initial values for the covariate effects and intercepts, which are all drawn from a standard normal distribution. I call the model and specify which values I want it to monitor. 

```{R Fit Model, eval = FALSE}
# Create a list of data for JAGS
jags_data <- list(
  # True presence at sites with confirmed presences
  Z = z,
  # Detection/nondetection data
  det_data = y,
  # Date
  cov1 = date_cov,
  # Veg Density - site level
  cov2 = veg_cov,
  # Background noise
  cov3 = db_cov,
  # Effort 
  cov4 = effort_cov,
  n_unit = nP,
  n_rep = nV
)

# Set initial values (optional)
init_func <- function(){
  list(
    a0 = rnorm(1, 0, 1),
    b0 = rnorm(1, 0, 1),
    b1 = rnorm(1, 0, 1),
    b2 = rnorm(1, 0, 1),
    b3 = rnorm(1, 0, 1),
    b4 = rnorm(1, 0, 1)
  )
}

# Run JAGS model on the same model we specify in the Fit_JAGS_Detection_Model2_HabChap markdown
sim_fit1 <- jags(data = jags_data, 
                 model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_HabDetMod2.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = ni, 
                 n.burnin = nb, 
                 n.chains = nc,
                 n.thin = nt,
                 parallel = TRUE,
                 inits = init_func)
#saveRDS(sim_fit1, file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Simulations/Test_JAGS_RecoverParams_GlobalDetMod2.Rdata")
```

## Model outputs from Simulated Data and Parameters 

Now let's look at the output of this model. What I'm looking for here is to see if my model converged (meaning that it is specified correctly and the simulated data makes sense) and whether or not it was able to recover the parameters I used to generate the data. 
```{R Evaluate Convergence Sim1}
# Read in the model that we saved after running the above code
sim_fit1 <- readRDS("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Simulations/Test_JAGS_RecoverParams_GlobalDetMod2.Rdata")

# Evaluate convergence
print(sim_fit1)
# plot traceplots
tracedens_jags(sim_fit1)
```

It looks like this model converged nicely. Let's check and see if it effectively recovered the parameters. I'm going to take a look at the values themselves, then plot the true values on top of the posterior distribution to see how close to the center they are. 
```{R Evaluate Outputs Sim1}
# Get a summary of the results
summary(sim_fit1)
# Did we recover our parameters?
beta
```
```{R Plot Chains Sim1}
# Plot posterior distribution
jags_chains1 <- jags_df(sim_fit1)

chains_viol1 <- jags_chains1 %>% select(b1,b2,b3,b4)
colnames(chains_viol1) <- c("date","veg_density","background_noise","effort")
true_vals <- data.frame(beta,covariate=c("date","veg_density","background_noise","effort"))
chains_viol_long1 <- chains_viol1 %>% pivot_longer(cols = c("date","veg_density","background_noise","effort"),names_to = "Parameter", values_to = "Values")

# Create the violin plot
ggplot() +
  geom_violin(data=chains_viol_long1, aes(x = Parameter, y = Values,fill = Parameter),trim = FALSE) +
  labs(title = "Posterior Distributions and True Parameters") +
  scale_fill_manual(values = c("date"=palette_5[1], "background_noise"=palette_5[2], "effort" = palette_5[3], "veg_density" = palette_5[4])) + theme(axis.text.x = element_blank())+
  geom_point(data = true_vals, aes(x=covariate, y = beta))+
  theme_minimal()
```


So it looks like my model is performing well on the data I simulated. But the real data that I have has NA values, either from where a vegetation survey wasn't conducted or where the ARU cut out early in the season. Let's try running the same model but with a little bit of "mess" in the data. 

## Add in NA values to the data

```{R Add in Mess and Sub Random Data for NAs, eval = FALSE}
# Copy over values for latent state from previous data
psi0_2 <- psi0
p0_2 <- p0
beta2 <- beta

# Now let's try the same model with some "messiness" i.e. holes for NAs in the data
# Create some holes in the samples to simulate NA data
# select randomly which data will be NA
mess_y <- sample(1:nP, 16)
sort(mess_y)
y2 <- y
# Change this data to NA
y2[mess_y, 3:6] <- NA

# Create new covariate values with some "mess" in them
date_cov2 <- date_cov
veg_cov2 <- veg_cov
db_cov2 <- db_cov
db_cov2[mess_y, 3:6] <- NA
effort_cov2 <- effort_cov
effort_cov2[mess_y, 3:6] <- NA

# Copy over probability of presence and actual presence
psi2 <- psi
z2 <- z

# Call the model again
# Create a list of data for JAGS
jags_data2 <- list(
  # True presence at sites with confirmed presences
  Z = z2,
  # Detection/nondetection data
  det_data = y2,
  # Date
  cov1 = date_cov2,
  # Veg Density - site level
  cov2 = veg_cov2,
  # Background noise
  cov3 = db_cov2,
  # Effort 
  cov4 = effort_cov2,
  n_unit = nP,
  n_rep = nV
)

# Run JAGS model
jags_fit_test_mess <- jags(data = jags_data2, 
                 model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_HabDetMod2.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = ni, 
                 n.burnin = nb, 
                 n.chains = nc,
                 n.thin = nt,
                 parallel = TRUE,
                 inits = init_func)
saveRDS(jags_fit_test_mess, file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Simulations/Test_JAGS_RecoverParams_WMess_GlobalDetMod2.Rdata")
```

Once again, let's evaluate the convergence and outputs of the model. 
```{R Evaluate Convergence Output Sim2}
sim_fit2 <-readRDS("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Simulations/Test_JAGS_RecoverParams_WMess_GlobalDetMod2.Rdata")

# Evaluate convergence
tracedens_jags(sim_fit2)
# all of the covariates converged

# Get a summary of the results
print(sim_fit2)
# Did we recover our parameters?
beta2
# Retrieved the parameters but not as well?
# Add on violin plots

jags_chains_global2 <- jags_df(sim_fit2)

chains_viol2 <- jags_chains_global2 %>% select(b1,b2,b3,b4)
colnames(chains_viol2) <- c("date","veg_density","background_noise","effort")
true_vals2 <- data.frame(beta2,covariate=c("date","veg_density","background_noise","effort"))
chains_viol_long2 <- chains_viol2 %>% pivot_longer(cols = c("date","veg_density","background_noise","effort"),names_to = "Parameter", values_to = "Values")

# Create the violin plot
ggplot() +
  geom_violin(data=chains_viol_long2, aes(x = Parameter, y = Values,fill = Parameter),trim = FALSE) +
  labs(title = "Posterior Distribution w/ NAs") +
  scale_fill_manual(values = c("date"=palette_5[1], "background_noise"=palette_5[2], "effort" = palette_5[3], "veg_density" = palette_5[4])) + theme(axis.text.x = element_blank())+
  geom_point(data = true_vals2, aes(x=covariate, y = beta))+
  theme_minimal()

```

From this, it looks like the NAs are biasing the parameter towards no effect and failing to find significance, especially when the underlying parameter was already small.

However, the way we treated NA values in the previous simulation was to sample from a distribution to fill them in. Another way to handle them is to simply skip over them in the model. Let's try this and see what we get. 

## Excluding NAs in Model 

```{R Excluding NAs, eval = FALSE}
get_na <- function(x){(max(which(!is.na(x))))}
# Apply this to our data
miss <- apply(y2, 1, get_na)

cat("
  model{
    # Loop through each sampling unit
    for(i in 1:n_unit){
      Z[i] ~ dbern(psi[i])  # Presence or absence drawn from a bernoulli distribution with occupancy probability psi  
      logit(psi[i]) <- a0
      
      for(j in 1:miss[i]){
        det_data[i,j] ~  dbern(Z[i]*theta[i,j])  # Create detection data drawn from a bernoulli distribution incorporating the true presence (Z) with probability of detection theta
        logit(theta[i,j]) <- b0 + b1*cov1[i,j] + b2*cov2[i] + b3*cov3[i,j] + b4*cov4[i,j]
        
      }
    }
    
    # Priors
    a0 ~ dlogis(0, 1)
    b0 ~ dlogis(0, 1)
    b1 ~ dnorm(0, 1)
    b2 ~ dnorm(0, 1)
    b3 ~ dnorm(0, 1)
    b4 ~ dnorm(0, 1)
  }
  ", file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_HabDetMod2-SkipNA.txt")

# Create a list of data for JAGS
jags_data3 <- list(
  # True presence at sites with confirmed presences
  Z = z2,
  # Detection/nondetection data
  det_data = y2,
  # Date
  cov1 = date_cov2,
  # Veg Density - site level
  cov2 = veg_cov2,
  # Background noise
  cov3 = db_cov2,
  # Effort 
  cov4 = effort_cov2,
  n_unit = nP,
  miss = miss
)

# Run JAGS model
jags_fit_test_mess2 <- jags(data = jags_data3, 
                 model.file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Model_Structure_Files/JAGS_HabDetMod2-SkipNA.txt",
                 parameters.to.save = c("a0", "b0", "b1", "b2", "b3", "b4"),
                 n.iter = ni, 
                 n.burnin = nb, 
                 n.chains = nc,
                 n.thin = nt,
                 parallel = TRUE,
                 inits = init_func)
saveRDS(jags_fit_test_mess2, file = "./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Simulations/Test_JAGS_RecoverParams_WMess_GlobalDetMod2-SkipNA.Rdata")
```

```{R Evaluate Convergence Outputs Sim3}
sim_fit3 <-readRDS("C:/Users/annak/OneDrive/Documents/UM/Research/Coding_Workspace/Cuckoo-Research/./R_Scripts/2_Analyses/Habitat_Chapter/Detection_Submodel/Models_Ran_Outputs/Simulations/Test_JAGS_RecoverParams_WMess_GlobalDetMod2-SkipNA.Rdata")

# Evaluate convergence
tracedens_jags(sim_fit3)
# all of the covariates converged

# Get a summary of the results
print(sim_fit3)
# Did we recover our parameters?
beta2
# Retrieved the parameters but not as well?
# Add on violin plots

jags_chains_global3 <- jags_df(sim_fit3)

chains_viol3 <- jags_chains_global3 %>% select(b1,b2,b3,b4)
colnames(chains_viol3) <- c("date","veg_density","background_noise","effort")
true_vals3 <- data.frame(beta2,covariate=c("date","veg_density","background_noise","effort"))
chains_viol_long3 <- chains_viol3 %>% pivot_longer(cols = c("date","veg_density","background_noise","effort"),names_to = "Parameter", values_to = "Values")

# Create the violin plot
ggplot() +
  geom_violin(data=chains_viol_long3, aes(x = Parameter, y = Values,fill = Parameter),trim = FALSE) +
  labs(title = "MCMC Chains Simulated Data w/ NAs Skipped") +
  scale_fill_manual(values = c("date"=palette_5[1], "background_noise"=palette_5[2], "effort" = palette_5[3], "veg_density" = palette_5[4])) + theme(axis.text.x = element_blank())+
  geom_point(data = true_vals3, aes(x=covariate, y = beta))+
  theme_minimal()

```

It doesn't look like subbing in NAs vs just skipping the NA values makes a huge difference. Skipping the NAs just pulls the posterior distribution towards the center of that distribution. 

It does, however, look like having 16/107 (~15%) NAs is masking our ability to see an effect. In the data with no NAs, all parameters showed a significant effect, but once we masked out the NAs, background dB (the one with the lowest true effect size) now overlapped zero more strongly, with an f value going from 0.96 to 0.67, meaning that if we just had the data with the NAs masked, we would have a false negative after running the models. 

